system:
  _target_: rcognita.systems.SysInvertedPendulum
  dim_input: 1
  dim_output: 2
  sys_type: diff_eqn
  dim_state: 2
  dim_disturb: 2
  pars: ${get:\[${.m__IGNORE__}\, ${.g__IGNORE__}\, ${.l__IGNORE__}\]}
  m__IGNORE__: 1
  g__IGNORE__: 9.8
  l__IGNORE__: 1
  M_min__IGNORE__: -20.0
  M_max__IGNORE__: 20.0
  action_bounds__IGNORE__: ${get:numpy.array\(\[\[${.M_min__IGNORE__}\, ${.M_max__IGNORE__}\]\]\)}
scenario:
  _target_: rcognita.scenarios.MonteCarloScenario
  running_objective: ${same:running_objective}
  simulator: ${same:simulator}
  is_log: false
  howanim: None
  state_init: ${initial_conditions.state_init}
  action_init: ${initial_conditions.action_init}
  N_episodes: 10
  N_iterations: 20
  speedup: 75
  controller: ${same:controller}
  observation_target: ${system_specific.observation_target}
  observation_components_naming: ${observation_naming.observation}
  total_objective_threshold: ${system_specific.total_objective_threshold}
simulator:
  _target_: rcognita.simulator.Simulator
  system: ${same:system}
  sys_type: ${system.sys_type}
  state_init: ${initial_conditions.state_init}
  action_init: ${initial_conditions.action_init}
  disturb_init: ${get:None}
  time_start: 0
  time_final: ${system_specific.time_final}
  sampling_time: ${controller.sampling_time}
  max_step: ${get:${controller.sampling_time} / 10.}
  first_step: 1.0e-06
  atol: 1.0e-05
  rtol: 0.001
  ode_backend: CASADI
nominal_controller:
  _target_: rcognita.controllers.NominalControllerInvertedPendulum
  time_start: ${simulator.time_start}
  sampling_time: 0.01
  action_bounds: ${system.action_bounds__IGNORE__}
  controller_gain: 20
controller:
  actor:
    model:
      _target_: rcognita.models.GaussianPDFModel
      dim_observation: ${controller.actor.dim_input}
      dim_action: ${controller.actor.dim_output}
      use_derivative: ${controller.actor.is_use_derivative}
      diag_scale_coef: ${system_specific.diag_scale_coef }
      weight_max: 20.0
      weight_min: -20.0
    optimizer:
      batch_sampler: {}
      _target_: rcognita.optimizers.TorchDataloaderOptimizer
      opt_options:
        lr: 5.0
      opt_method: ${get:torch.optim.Adam}
      model: ${same:controller.actor.model}
      sheduler_method: ${get:torch.optim.lr_scheduler.StepLR}
      sheduler_options:
        gamma: 0.25
        step_size: 5
    _target_: rcognita.actors.ActorPG
    system: ${same:system}
    N_episodes: ${scenario.N_episodes}
    N_iterations: ${scenario.N_iterations}
    dim_output: ${system.dim_input}
    dim_input: ${system.dim_output}
    discount_factor: 1.0
    action_bounds: ${system.action_bounds__IGNORE__}
    action_init: ${scenario.action_init}
    state_init: ${scenario.state_init}
    critic: ${same:controller.critic}
    observation_target: ${system_specific.observation_target}
    running_objective: ${running_objective}
    device: cuda:0
    is_use_derivative: ${system_specific.is_use_derivative}
  critic:
    _target_: rcognita.critics.CriticTrivial
    running_objective: ${same:running_objective}
    optimizer:
      opt_options:
        lr: 1.0e-05
    td_n: 10
    batch_size: 50
    discount_factor: 0.94
  episode_data_buffer:
    _target_: rcognita.data_buffers.ObservationActionObjectiveAccStatsDataset
    system: ${same:system}
    is_use_derivative: ${system_specific.is_use_derivative}
    is_cat_action: true
    is_tail_sum_running_objectives: false
  name__IGNORE__: pg
  _target_: rcognita.controllers.RLController
  time_start: ${simulator.time_start}
  sampling_time: ${system_specific.sampling_time}
  is_fixed_critic_weights: false
  critic_period: ${get:${.critic_period_multiplier__IGNORE__} * ${.sampling_time}}
  action_bounds: ${system.action_bounds__IGNORE__}
  critic_period_multiplier__IGNORE__: 1
running_objective:
  model:
    _target_: rcognita.models.ModelQuadForm
    weights: ${get:numpy.diag\(${.R1_diag__IGNORE__}\)}
    R1_diag__IGNORE__: ${get:\[10.\, 3.\, 0.\]}
  _target_: rcognita.objectives.RunningObjective
initial_conditions:
  state_init: ${get:numpy.array\(\[numpy.pi\, 0.\]\) + numpy.array\(\[${.x1}\, 0.\]\)}
  action_init: ${get:numpy.ones\(${system.dim_input}\)}
  x1: ${get:numpy.random.normal\(0.\, 0.05\)}
observation_naming:
  observation: ${get:\[__QUOTATION__angle__QUOTATION__\, __QUOTATION__angle_dot__QUOTATION__\]}
system_specific:
  name: inv_pendulum
  observation_target: ${get:\[\]}
  sampling_time: 0.01
  time_final: 10
  grid_dim__IGNORE__: 3
  M_min__IGNORE__: ${system.M_min__IGNORE__}
  M_max__IGNORE__: ${system.M_max__IGNORE__}
  actions_grid: ${get:numpy.vstack\(map\(numpy.ravel\, numpy.meshgrid\(numpy.linspace\(${.M_min__IGNORE__}\,
    ${.M_max__IGNORE__}\, ${.grid_dim__IGNORE__}\)\)\)\)}
  safe_decay_param_ex_post: 200.0
  safe_decay_param_predictive: 5000.0
  critic_penalty_param: 0
  critic_regularization_param: 0
  is_dynamic_decay_rate: true
  lb_parameter: 1.0e-06
  ub_parameter: 1000.0
  is_use_derivative: false
  data_buffer_size: 500
  diag_scale_coef: 3.5
  calf_data_buffer_size: 8
  single_weight_min: 1.0e-06
  single_weight_max: 100.0
  fps: 15
  mpc_prediction_horizon: 10
  total_objective_threshold: 5000
animator:
  _target_: rcognita.visualization.vis_inverted_pendulum.AnimatorInvertedPendulum
  scenario: ${same:scenario}
  fps: ${system_specific.fps}
  max_video_length: 60
  animation_max_size_mb: 200
callbacks:
- rcognita.callbacks.HistoricalObjectiveCallback
- rcognita.callbacks.TotalObjectiveCallback
- rcognita.callbacks.CriticObjectiveCallback
- rcognita.callbacks.CalfCallback
- rcognita.callbacks.HistoricalObservationCallback
- rcognita.callbacks.QFunctionModelSaverCallback
- rcognita.callbacks.CalfWeightsCallback
- rcognita.callbacks.CriticWeightsCallback
- rcognita.callbacks.InspectReferrersCallback
- rcognita.callbacks.PolicyGradientObjectiveSaverCallback
disallow_uncommitted__IGNORE__: false
seed: 1
