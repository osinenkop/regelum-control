_target_: regelum.controller.RLController

name%%: sarsa

defaults:
  - policy: policy_rl_torch
  - critic: critic

action_bounds: $ system_specific.action_bounds
max_data_buffer_size: 10000
sampling_time: $ system_specific.sampling_time
is_critic_first: False
running_objective: ~ running_objective
discount_factor: $ scenario.discount_factor
critic_optimization_event: compute_action
policy_optimization_event: compute_action
data_buffer_nullify_event: reset_episode
device%%: cpu

critic:
  is_on_policy: True
  is_same_critic: False
  is_value_function: False
  device: $ controller.device%%
  optimizer_config: 
    _target_: regelum.OptimizerConfig
    kind: tensor
    opt_method: = torch.optim.Adam
    opt_options: 
      lr: 0.001
    config_options:
      n_epochs: 2
      is_reinstantiate_optimizer: False
      data_buffer_sampling_method: iter_batches
      data_buffer_sampling_kwargs: 
        batch_sampler: = regelum.data_buffers.batch_sampler.RollingBatchSampler
        dtype: = torch.FloatTensor
        batch_size: 20
        n_batches: 20
        mode: backward

policy:
  device: $ controller.device%%
  epsilon_random: True
  epsilon_random_parameter: 0.05
  prediction_horizon: 0 
  algorithm: rql

  optimizer_config: 
    _target_: regelum.OptimizerConfig
    kind: tensor
    opt_method: = torch.optim.Adam
    opt_options: 
      lr: 0.01
    config_options:
      n_epochs: 10
      is_reinstantiate_optimizer: True
      data_buffer_sampling_method: iter_batches
      data_buffer_sampling_kwargs: 
        batch_sampler: = regelum.data_buffers.batch_sampler.RollingBatchSampler
        dtype: = torch.FloatTensor
        batch_size: 1
        mode: backward
        n_batches: 1

        