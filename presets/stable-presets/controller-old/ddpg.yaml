_target_: regelum.pipeline.RLPipeline

name%%: ddpg

defaults:
  - policy: policy_ddpg
  - critic: critic

action_bounds: $ system_specific.action_bounds
sampling_time: $ system_specific.sampling_time
is_critic_first: True
running_objective: ~ running_objective
discount_factor: 1.0
critic_optimization_event: = regelum.Event.reset_iteration
policy_optimization_event: = regelum.Event.reset_iteration
data_buffer_nullify_event: = regelum.Event.reset_iteration
simulator: ~ simulator
N_episodes: 4 
N_iterations: 100
constraint_parser: ~ constraint_parser
observer: $ system_specific.observer

device%%: cpu


critic:
  is_on_policy: True
  is_same_critic: False
  is_value_function: False
  device: $ pipeline.device%%
  td_n: 1
  optimizer_config: 
    _target_: regelum.OptimizerConfig
    kind: tensor
    opt_method: = torch.optim.Adam
    opt_options: 
      lr: 0.001
    config_options:
      n_epochs: 50
      is_reinstantiate_optimizer: True
      data_buffer_sampling_method: iter_batches
      data_buffer_sampling_kwargs: 
        batch_sampler: = regelum.data_buffers.batch_sampler.EpisodicSampler
        dtype: = torch.FloatTensor
  model:
    dim_input: = ${system_specific.dim_observation} + ${system_specific.dim_action}
    dim_output: 1
    dim_hidden: 100
    n_hidden_layers: 4
    is_force_infinitesimal: True
    is_bias: True
    force_positive_def: False

policy:
  model:
    dim_hidden: 4
    n_hidden_layers: 2
  optimizer_config: 
    opt_options:
      lr: 0.1
    config_options:
      is_reinstantiate_optimizer: False
      n_epochs: 1
  device: $ pipeline.device%%