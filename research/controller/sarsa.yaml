#############################
###---A "defaults" list---###
#############################

defaults:
  - actor: rql
  - critic: action_observation
  - override actor/optimizer: torch_projective
  - override actor/model: weight_container
  - override critic/model: dqn
  - override critic/optimizer: torch

#################################
###---Constructor arguments---###
#################################
_target_: rcognita.controllers.RLController

time_start: $ simulator.time_start
sampling_time: 1.0E-2
is_fixed_critic_weights: false

F_min%%: -300.
F_max%%: 300.
M_min%%: -100.
M_max%%: 100.

critic_period: = ${.critic_period_multiplier%%} * ${.sampling_time}
action_bounds: = numpy.array([[${.F_min%%}, ${.F_max%%}], [${.M_min%%}, ${.M_max%%}]])

critic_period_multiplier%%: 1



actor:
  epsilon_greedy: true
  epsilon_greedy_parameter: 0.1
  predictor:
    prediction_horizon: 0.

critic:
  optimizer:
    opt_options:
      lr: 1.0E-7
    iterations: 1

