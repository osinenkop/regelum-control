<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Description</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="vpg.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="description" link="x4-3000" md-heading="Description" name="md-heading" type="h3"/></head><body>
<!-- l. 45 -->
<!-- l. 46 --><p class="noindent">The underlying theory of the VPG algorithm is captured by the relationship between the
gradient of the expected total cost and the policy parameters \(\theta \). This relationship is formally
stated by the policy gradient theorem, which is mathematically represented as \begin {equation}  \label {eqn_vpg_log_lh_trick} \nabla _{\theta } \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} = \E [\transit , \policy ^{\theta }]{ \sum _{t = 0}^{\infty }\gamma ^t\Advan ^{\policy ^{\theta }} (\State _t, \Action _t)\nabla _{\theta } \log \policy ^{\theta }(\Action _t \mid \State _t)}  \end {equation}<a id="x4-3001r2"></a> In practice,
\eqref{eqn_vpg_log_lh_trick} can be approximated using Monte Carlo methods. Specifically, by sampling \(M\) trajectories
(referred to as <span class="cmti-12">episodes</span>), which are generated using the policy \(\pi ^{\theta }\), and then averaging
across these episodes, we obtain the following approximation: \begin {equation}  \label {eqn_vpg_objective_est0} \begin {aligned} &amp;\E [\transit , \policy ^{\theta }]{ \sum _{t = 0}^{\infty }\gamma ^t\Advan ^{\policy ^{\theta }} (\State _t, \Action _t)\nabla _{\theta } \log \policy ^{\theta }(\Action _t \mid \State _t)} \approx \\ &amp;\quad \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \Advan ^{\policy ^{\theta }}(\State _t^j, \Action _t^j) \nabla _{\theta } \log \policy ^{\theta }(\Action _t^j \mid \State _t^j), \end {aligned}  \end {equation}<a id="x4-3002r3"></a> where \(T\) denotes the
length of an episode, and \(\State _t^j\) and \(\Action _t^j\) represent the state and action at time \(t\) of the \(j\)-th
episode, respectively. However, the advantage function \(\Advan ^{\policy ^{\theta }}(s, a)\) is generally unknown
and must be estimated. This estimation can be performed by minimizing the
temporal difference (TD) loss for the value function \(\Value ^{\policy ^{\theta }}\), as described by the following
equation: \begin {equation}  \label {eqn_td_loss} \sum _{j=1}^M\sum _{t=0}^{T-1 - N_{\text {TD}}}\left (\hat \Value ^w(\State _t^j) - \sum _{t'=t}^{t + N_{\text {TD}} - 1} \gamma ^{t'-t}\Cost (\State _{t'}^j, \Action _{t'}^j) - \hat \Value ^w(\State _{t + N_{\text {TD}}}^j)\right )^2 \ra \min _{w},  \end {equation}<a id="x4-3003r4"></a> where \(N_{\text {TD}}\) is the TD window, a hyperparameter. After optimization, we
achieve an approximation of the value function, \(V^{\pi _{\theta }} \approx \hat {V}^w\), which allows us to estimate the
advantage function as follows: \begin {equation}  \label {eqn_advan_est} \Advan ^{\policy ^{\theta }}(\State _t, \Action _t) \approx \Cost (\State _t, \Action _t) + \gamma \hat \Value ^w(\State _{t+1}) - \hat \Value ^w(\State _t) =: \hat \Advan ^{w}(\State _t, \Action _t) ,  \end {equation}<a id="x4-3004r5"></a> Substituting this estimated advantage into \eqref{eqn_vpg_objective_est0}
yields: \begin {equation}  \label {eqn_vpg_objective_est} \begin {aligned} &amp;\E [\transit , \policy ^{\theta }]{ \sum _{t = 0}^{\infty }\gamma ^t\Advan ^{\policy ^{\theta }} (\State _t, \Action _t)\nabla _{\theta } \log \policy ^{\theta }(\Action _t \mid \State _t)} \approx \\ &amp;\sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \hat \Advan ^{w}(\State _t^j, \Action _t^j) \nabla _{\theta } \log \policy ^{\theta }(\Action _t^j \mid \State _t^j), \end {aligned}  \end {equation}<a id="x4-3005r6"></a> We come up with the final version of VPG algorithm in the subsequent
section.
                                                                                   
                                                                                   
</p>
<!-- l. 81 -->
<!-- l. 81 --><p class="indent"> <a id="tailvpgli3.html"></a> </p>
</body>
</html>