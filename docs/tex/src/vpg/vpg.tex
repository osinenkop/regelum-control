\documentclass[12pt,twoside]{../../mitthesis}
\input{../../packages.tex}

\begin{document}
\section*{Problem statement}
Vanilla Policy Gradient (VPG) is a policy gradient algorithm that solves the following problem:

\begin{equation}
    \label{eqn_ppo_problem}
    \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} \ra \min_{\theta},
\end{equation}
where:
\begin{enumerate}
    \item $\gamma \in (0, 1)$ is the \textit{discount factor} of the algorithm. 
    \item  $\transit : \states \times \actions \times \states \ \rightarrow \ \R_{\geq 0}$ is the \textit{transition probability density function} of the environment. The function $\transit(\bullet \mid \state, \action)$ is the probability density of the next state conditioned on the current state $\state$ and the current action $\action$:
    $$
        \State_{t+1} \sim \transit(\bullet \mid \State_t, \Action_t), \quad \text{for all timesteps } t \in \{0, 1, \dots, T-1\} 
    $$
    It is also assumed that the initial state $\State_0$ for $t = 0$ is sampled from some distribution with an unconditional density $\transit_0 : \states \ \rightarrow \ \R_{\geq 0}$:
    $$
        \State_0 \sim \transit_0(\bullet)
    $$
    \item $\policy^{\theta} : \actions \times \states \rightarrow \R_{\geq 0}$ is the \textit{stochastic policy} of the agent that is paramenterized by weights $\theta$. Precicely, $\policy^{\theta}(\bullet \mid \state)$ is the probability density of the action conditioned on the current state $\state$:
    $$
        \Action_t \sim \policy^{\theta}(\bullet \mid \State_t)
    $$
    \item $\Cost(s, a)$ denotes the cost associated with the current state $\state$ and action $\action$
\end{enumerate}

\section*{Notation}
The notations used throughout this document are defined as follows:

\begin{enumerate}
    \item The value function of policy $\policy^{\theta}$ is denoted by $V^{\policy^{\theta}}(s)$ and is defined as the expected total cost when starting in state $s$ and following policy $\policy^{\theta}$ thereafter:
    $$
    \Value^{\policy^{\theta}}(s) =  \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t}) \mid \State_0 = \state}
    $$ 
    \item The Q-function of policy $\policy^{\theta}$ for a state-action pair $(s, a)$ is denoted by $Q^{\policy^{\theta}}(s, a)$ and represents the expected total cost after taking action $a$ in state $s$ and thereafter following policy $\policy^{\theta}$: 
    $$
    Q^{\policy^{\theta}}(s, a) = \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t}) \mid \State_0 = \state, \Action_0 = \action}
    $$
    \item The advantage function is defined as $\Advan^{\policy^{\theta}}(s, a) = Q^{\policy^{\theta}}(s, a) - \Value^{\policy^{\theta}}(s)$.
    \item The Kullback-Leibler (KL) divergence, denoted by $\kldiv{\bullet_1}{\bullet_2}$ measures the difference between two probability distributions $\bullet_1$ and $\bullet_2$.
\end{enumerate}

\section*{Description}
The underlying theory of the VPG algorithm is captured by the relationship between the gradient of the expected total cost and the policy parameters $\theta$. 
This relationship is formally stated by the policy gradient theorem, which is mathematically represented as
\begin{equation}
    \label{eqn_vpg_log_lh_trick}
    \nabla_{\theta} \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} =  \E[\transit, \policy^{\theta}]{ \sum_{t = 0}^{\infty}\gamma^t\Advan^{\policy^{\theta}} (\State_t, \Action_t)\nabla_{\theta} \log \policy^{\theta}(\Action_t \mid \State_t)}
\end{equation}
In practice, \eqref{eqn_vpg_log_lh_trick} can be approximated using Monte Carlo methods. Specifically, by sampling $M$ trajectories (referred to as \textit{episodes}), which are generated using the policy $\pi^{\theta}$, and then averaging across these episodes, we obtain the following approximation:
\begin{equation}
    \label{eqn_vpg_objective_est0}
\E[\transit, \policy^{\theta}]{ \sum_{t = 0}^{\infty}\gamma^t\Advan^{\policy^{\theta}} (\State_t, \Action_t)\nabla_{\theta} \log \policy^{\theta}(\Action_t \mid \State_t)} \approx \frac{1}{M} \sum_{j = 1}^M \sum_{t=0}^{T-1} \gamma ^ t \Advan^{\policy^{\theta}}(\State_t^j, \Action_t^j) \nabla_{\theta} \log \policy^{\theta}(\Action_t^j \mid \State_t^j),
\end{equation}
where $T$ denotes the length of an episode, and  $\State_t^j$  and $\Action_t^j$ represent the state and action at time $t$ of the $j$-th episode, respectively.
However, the advantage function $\Advan^{\policy^{\theta}}(s, a)$ is generally unknown and must be estimated. This estimation can be performed by minimizing the temporal difference (TD) loss for the value function  $\Value^{\policy^{\theta}}$, as described by the following equation:
\begin{equation}
    \label{eqn_td_loss}
    \frac{1}{M}\sum_{j=1}^M\sum_{t=0}^{T-1 - N_{\text{TD}}}\left(\hat\Value^w(\State_t^j) - \sum_{t'=t}^{t + N_{\text{TD}} - 1} \gamma^{t'-t}\Cost(\State_{t'}^j, \Action_{t'}^j) - \hat\Value^w(\State_{t + N_{\text{TD}}}^j)\right)^2 \ra \min_{w},
\end{equation}
where $N_{\text{TD}}$ is the TD window, a hyperparameter.
After optimization, we achieve an approximation of the value function, $V^{\pi_{\theta}} \approx \hat{V}^w$, which allows us to estimate the advantage function as follows: 
\begin{equation}
    \label{eqn_advan_est}
    \Advan^{\policy^{\theta}}(\State_t, \Action_t) \approx \Cost(\State_t, \Action_t) + \gamma \hat\Value^w(\State_{t+1}) - \hat\Value^w(\State_t) =: \hat\Advan^{w}(\State_t, \Action_t) ,
\end{equation}
Substituting this estimated advantage into \eqref{eqn_vpg_objective_est0} yields:
\begin{equation}
    \label{eqn_vpg_objective_est}
    \E[\transit, \policy^{\theta}]{ \sum_{t = 0}^{\infty}\gamma^t\Advan^{\policy^{\theta}} (\State_t, \Action_t)\nabla_{\theta} \log \policy^{\theta}(\Action_t \mid \State_t)} \approx \frac{1}{M} \sum_{j = 1}^M \sum_{t=0}^{T-1} \gamma ^ t \hat\Advan^{w}(\State_t^j, \Action_t^j) \nabla_{\theta} \log \policy^{\theta}(\Action_t^j \mid \State_t^j),
\end{equation}
We come up with the final version of VPG algorithm in the subsequent section.
\section*{VPG algorithm}
\begin{algorithm}
    \caption{Vanilla Policy Gradient (VPG)}
    \label{alg:my-alg}
    \begin{algorithmic}
    \STATE {\bfseries Input:} $\theta_1$ (initial policy weights)
    \FOR {learning iteration $i := 1 \dots \mathcal I$}
        \FOR {episode $j := 1 \dots M$}
            \STATE obtain initial state $\State_0^{j}$
            \FOR {step $t := 0 \dots T - 1$}
                \STATE sample action $\Action_t^j \sim \policy^{\theta}(\bullet \mid \State_t^{j})$
                \STATE obtain state from transition function $\State_{t+1}^j \sim \transit(\bullet \mid \State_t^j, \Action_t^j)$
            \ENDFOR
        \ENDFOR
        \STATE Optimize critic $\hat\Value^{w}$ using gradient descent to minimize the temporal difference loss \eqref{eqn_td_loss}
        \STATE Estimate the advantages $\hat\Advan^{w}(\State_t, \Action_t)$ as described in \eqref{eqn_advan_est}  
        \STATE Perform a policy gradient step:
        $$
            \theta_{i+1} \la \theta_i - \frac{1}{M} \sum_{j = 1}^M \sum_{t=0}^{T-1} \gamma ^ t \hat\Advan^{w}(\State_t^j, \Action_t^j) \nabla_{\theta} \log \policy^{\theta}(\Action_t^j \mid \State_t^j)\rvert_{\theta = \theta_i}.
        $$
    \ENDFOR
    \STATE \RETURN Optimal policy $\policy^{\theta_{\mathcal I}}$
    \end{algorithmic}
\end{algorithm}


\end{document}