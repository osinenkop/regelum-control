% !TeX document-id = {fb298762-8474-4a7b-90a0-a0c749091c0f}
%!BIB program = biber
\documentclass[12pt,twoside]{../../mitthesis}
\input{../../packages.tex}

\begin{document}
\section*{Problem statement}
Proximal Policy Optimization (PPO) is a policy gradient algorithm that solves the following problem:

\begin{equation}
    \label{eqn_ppo_problem}
    \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} \ra \min_{\theta},
\end{equation}
where:
\begin{enumerate}
    \item $\gamma \in (0, 1)$ is the \textit{discount factor} of the algorithm. 
    \item  $\transit : \states \times \actions \times \states \ \rightarrow \ \R_{\geq 0}$ is the \textit{transition probability density function} of the environment. Precicely, $\transit(\bullet \mid \state, \action)$ is the probability density of the next state conditioned on the current state $\state$ and the current action $\action$:
    $$
        \State_{t+1} \sim \transit(\bullet \mid \State_t, \Action_t), \quad \text{for all timesteps } t \in \{0, 1, 2,\dots\} 
    $$
    It is also assumed that the initial state $\State_0$ for $t = 0$ is sampled from some distribution with an unconditional density $\transit_0 : \states \ \rightarrow \ \R_{\geq 0}$:
    $$
        \State_0 \sim \transit_0(\bullet)
    $$
    \item $\policy^{\theta} : \actions \times \states \rightarrow \R_{\geq 0}$ is the \textit{stochastic policy} of the agent that is paramenterized by weights $\theta$. Precicely, $\policy^{\theta}(\bullet \mid \state)$ is the probability density of the action conditioned on the current state $\state$:
    $$
        \Action_t \sim \policy^{\theta}(\bullet \mid \State_t)
    $$
    \item $\Cost(s, a)$ denotes the cost associated with the current state $\state$ and action $\action$
\end{enumerate}

\section*{Notation}

The notations used throughout this document are defined as follows:

\begin{enumerate}
    \item The value function of policy $\policy^{\theta}$ is denoted by $V^{\policy^{\theta}}(s)$ and is defined as the expected total cost when starting in state $s$ and following policy $\policy^{\theta}$ thereafter:
    $$
    \Value^{\policy^{\theta}}(s) =  \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t}) \mid \State_0 = \state}
    $$ 
    \item The Q-function of policy $\policy^{\theta}$ for a state-action pair $(s, a)$ is denoted by $Q^{\policy^{\theta}}(s, a)$ and represents the expected total cost after taking action $a$ in state $s$ and thereafter following policy $\policy^{\theta}$: 
    $$
    Q^{\policy^{\theta}}(s, a) = \E[\transit, \policy^{\theta}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t}) \mid \State_0 = \state, \Action_0 = \action}
    $$
    \item The advantage function is defined as $\Advan^{\policy^{\theta}}(s, a) = Q^{\policy^{\theta}}(s, a) - \Value^{\policy^{\theta}}(s)$.
    \item The Kullback-Leibler (KL) divergence, denoted by $\kldiv{\bullet_1}{\bullet_2}$ measures the difference between two probability distributions $\bullet_1$ and $\bullet_2$.
\end{enumerate}


\section*{Motivation}

Proximal Policy Optimization (PPO) evolved from Trust Region Policy Optimization (TRPO), and to understand the motivation behind the algorithm, we begin by considering a key theoretical result from \href{https://arxiv.org/abs/1502.05477}{TRPO paper}.

Let us consider the following update rule:
$$
    \theta_{i + 1} \la \arg\min_{\theta} \left( \E[\transit, \policy^{\theta_i}]{\sum_{t=0}^{\infty} \gamma ^ t \Advan^{\policy^{\theta_i}}(\State_t, \Action_t)   \frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t \mid \State_t)}} + C_i d_{\text{KL}}^{\max}\left(\policy^{\theta_i}\;\middle\|\;\policy^{\theta}\right) \right), 
$$
where 
\begin{enumerate}
    \item $i$ is the number of learning iteration
    \item $C_i := \frac{4 \gamma\max_{s \in \states, a \in \actions}|\Advan^{\policy^{\theta_i}}(s, a)|}{(1 - \gamma)^2}$
    \item $d_{\text{KL}}^{\max}\left(\policy^{\theta_i}\;\middle\|\;\policy^{\theta}\right)  = \max_{\state \in \states}\kldiv{\policy^{\theta_i}(\bullet \mid s)}{\policy^{\theta}(\bullet \mid s)}$
\end{enumerate}
According to this rule, it is guaranteed that the expected cumulative cost does not increase with each iteration:
$$
\E[\transit, \policy^{\theta_1}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} \geq \E[\transit, \policy^{\theta_2}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} \geq \E[\transit, \policy^{\theta_3}]{\sum_{t = 0}^{\infty} \gamma ^ t \Cost(\State_{t}, \Action_{t})} \geq ...
$$
Despite these guarantees, the straighforward application of this approach is faced with challenges such as computational complexity and slow convergence rates.
Nevertheless, we can derive significant interpretation of the result: when the optimization steps are sufficiently small, such that $\theta_i \rightarrow \theta_{i+1}$ yields a minimal increase in the maximal Kullback-Leibler divergence $d_{\text{KL}}^{\max}\left(\policy^{\theta_i} \,\middle\|\, \policy^{\theta_{i + 1}}\right)$, and while the objective 
\[
    \E[\transit, \policy^{\theta_i}]{\sum_{t=0}^{\infty} \gamma ^ t \Advan^{\policy^{\theta_i}}(\State_t, \Action_t)   \frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t \mid \State_t)}}
\]
is minimized, each subsequent iteration is ensured to be at least as good as the previous one.
\section*{Update rule}
To address practical difficulties and incorporate the proposed interpretation, Proximal Policy Optimization (PPO) introduces the following update rule:
\begin{equation}
    \theta_{i+1} \la \arg\min_{\theta}\E[\transit, \policy^{\theta_i}]{\sum_{t=0}^{\infty} \gamma ^ t \max\left(\Advan_t^{\policy^{\theta_i}} r_t(\theta, \theta_i), \Advan_t^{\policy^{\theta_i}}\operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(r_t(\theta, \theta_i) \right)\right)}
\end{equation}
where 
\begin{enumerate}
    \item $\varepsilon$ is a hyperparameter, typically set to $\varepsilon = 0.2$
    \item $r_t(\theta, \theta_i) =  \frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t \mid \State_t)}$
    \item $\Advan_t^{\policy^{\theta_i}} = \Advan^{\policy^{\theta_i}}(\State_t, \Action_t)$
\end{enumerate}
The rationale behind this objective function is as follows:

The first term within the $\max$ expression,
\begin{equation}
    \Advan_t^{\policy^{\theta_i}} r_t(\theta, \theta_i) = \Advan^{\policy^{\theta_i}}(\State_t, \Action_t) \frac{\policy^{\theta}(\Action_t | \State_t)}{\policy^{\theta_i}(\Action_t | \State_t)},
\end{equation}
is adopted from Trust Region Policy Optimization (TRPO). The second term,
\begin{equation}
    \Advan_t^{\policy^{\theta_i}}\operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(r_t(\theta, \theta_i) \right) = \Advan^{\policy^{\theta_i}}(\State_t, \Action_t) \operatorname{clip}_{1-\varepsilon}^{1 + \varepsilon}\left(\frac{\policy^{\theta}(\Action_t | \State_t)}{\policy^{\theta_i}(\Action_t | \State_t)}\right),
\end{equation}
modifies the objective by clipping the probability ratio to keep it within the interval $[1 - \varepsilon, 1 + \varepsilon]$. This modification removes the incentive for moving the ratio $\frac{\policy^{\theta}(\Action_t | \State_t)}{\policy^{\theta_i}(\Action_t | \State_t)}$ outside of this interval.

Finally, the maximum between the clipped and unclipped values is computed so that the final objective serves as a conservative or pessimistic bound on the unclipped objective. This approach ensures that changes in the probability ratio are considered only when they do not lead to an improvement in the objective, thereby providing a safeguard against overly aggressive updates.

\section*{Monte Carlo estimation of the update rule}
In practice, the objective 
$$
\begin{aligned}
&\E[\transit, \policy^{\theta_i}]{\sum_{t=0}^{\infty} \gamma ^ t \max\left(\Advan_t^{\policy^{\theta_i}} r_t(\theta, \theta_i), \Advan_t^{\policy^{\theta_i}}\operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(r_t(\theta, \theta_i) \right)\right)} = \\
&\E[\transit, \policy^{\theta_i}]{\sum_{t=0}^{\infty} \gamma ^ t \max\left(\Advan^{\policy^{\theta_i}}(\State_t, \Action_t)   \frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t \mid \State_t)}, \Advan^{\policy^{\theta_i}}(\State_t, \Action_t) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t \mid \State_t)}\right) \right)}
\end{aligned}
$$
can be approximated using Monte Carlo methods. Specifically, by sampling $M$ trajectories (referred to as \textit{episodes}), which are generated using the policy $\pi^{\theta_i}$, and then averaging across these episodes, we obtain the following approximation:
$$
    \frac{1}{M} \sum_{j = 1}^M \sum_{t=0}^{T-1} \gamma ^ t \max\left(\Advan^{\policy^{\theta_i}}(\State_t^j, \Action_t^j)   \frac{\policy^{\theta}(\Action_t^j \mid \State_t^j)}{\policy^{\theta_i}(\Action_t^j \mid \State_t^j)}, \Advan^{\policy^{\theta_i}}(\State_t^j, \Action_t^j) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\policy^{\theta}(\Action_t^j \mid \State_t^j)}{\policy^{\theta_i}(\Action_t^j \mid \State_t^j)}\right) \right),
$$
where $T$ denotes the length of an episode, and  $\State_t^j$  and $\Action_t^j$ represent the state and action at time $t$ of the $j$-th episode, respectively.
However, the advantage function $\Advan^{\policy^{\theta_i}}(s, a)$ is generally unknown and must be estimated. This estimation can be performed by minimizing the temporal difference (TD) loss for the value function  $\Value^{\policy^{\theta_i}}$, as described by the following equation:
\begin{equation}
    \label{eqn_td_loss}
    \sum_{j=1}^M\sum_{t=0}^{T-1 - N_{\text{TD}}}\left(\hat\Value^w(\State_t^j) - \sum_{t'=t}^{t + N_{\text{TD}} - 1} \gamma^{t'-t}\Cost(\State_{t'}^j, \Action_{t'}^j) - \gamma^{N_{\text{TD}}}\hat\Value^w(\State_{t + N_{\text{TD}}}^j)\right)^2 \ra \min_{w},
\end{equation}
where $N_{\text{TD}}$ is the TD window, a hyperparameter.
After optimization, we achieve an approximation of the value function, $V^{\pi_{\theta_i}} \approx \hat{V}^w$, which allows us to estimate the advantage function as follows: 
\begin{equation}
    \label{eqn_advan_est}
    \Advan^{\policy^{\theta_i}}(\State_t, \Action_t) \approx \Cost(\State_t, \Action_t) + \gamma \hat\Value^w(\State_{t+1}) - \hat\Value^w(\State_t) =: \hat\Advan^{w}(\State_t, \Action_t) ,
\end{equation}
Substituting this estimated advantage into the original objective yields the final PPO objective estimate:
\begin{equation}
    \label{eqn_ppo_objective_est}
    \frac{1}{M} \sum_{j = 1}^M \sum_{t=0}^{T-1} \gamma ^ t \max\left(\hat\Advan^{w}(\State_t, \Action_t)   \frac{\policy^{\theta}(\Action_t \mid \State_t)}{\policy^{\theta_i}(\Action_t^j \mid \State_t^j)}, \hat\Advan^{w}(\State_t^j, \Action_t^j) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\policy^{\theta}(\Action_t^j \mid \State_t^j)}{\policy^{\theta_i}(\Action_t^j \mid \State_t^j)}\right) \right)
\end{equation}

\section*{PPO algorithm}
\begin{algorithm}
    \caption{Proximal Policy Optimization (PPO)}
    \label{alg:my-alg}
    \begin{algorithmic}[1]
    \STATE {\bfseries Input:} $\theta_1$ (initial policy weights)
    \FOR {learning iteration $i := 1 \dots \mathcal I$}
        \FOR {episode $j := 1 \dots M$}
            \STATE obtain initial state $\State_0^{j}$
            \FOR {step $t := 0 \dots T - 1$}
                \STATE sample action $\Action_t^j \sim \policy^{\theta}(\bullet \mid \State_t^{j})$
                \STATE obtain state from transition function $\State_{t+1}^j \sim \transit(\bullet \mid \State_t^j, \Action_t^j)$
            \ENDFOR
        \ENDFOR
        \STATE Optimize critic $\hat\Value^{w}$ using gradient descent to minimize the temporal difference loss \eqref{eqn_td_loss}
        \STATE Estimate the advantages $\hat\Advan^{w}(\State_t, \Action_t)$ as described in \eqref{eqn_advan_est} 
        \STATE Update policy weights $\theta_{i + 1} = \arg\min$ \eqref{eqn_ppo_objective_est} by performing gradient descent 
    \ENDFOR
    \STATE \RETURN Optimal policy $\policy^{\theta_{\mathcal I}}$
    \end{algorithmic}
\end{algorithm}

\end{document}