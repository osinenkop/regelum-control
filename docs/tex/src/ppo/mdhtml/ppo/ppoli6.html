<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>PPO algorithm</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ppo.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="ppo-algorithm" link="x7-6000" md-heading="PPO algorithm" name="md-heading" type="h3"/></head><body>
<!-- l. 129 -->
<p><a id="x7-6001r1"></a>
</p><!-- l. 131 --><p class="indent"> </p><figure class="float" id="x7-6002r1"><span id="proximal-policy-optimization-ppo"></span><span></span>
<span class="cmbx-12">Algorithm 1:</span> Proximal Policy Optimization (PPO)
   
   <a id="x7-6003"></a>
</figure><div class="algorithmic">
<a id="x7-6004r1"></a>
<span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> 
      </span><span class="cmbx-12">Input:</span>
      \(\theta _1\)
      (initial
      policy
      weights)
      <a id="x7-6005r2"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">for</span> learning iteration \(i := 1 \dots \mathcal I\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x7-6006r3"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">for</span> episode \(j := 1 \dots M\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x7-6007r4"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;"> </span>
          obtain
          initial
          state
          \(\State _0^{j}\)
          <a id="x7-6008r5"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">for</span> step \(t := 0 \dots T - 1\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x7-6009r6"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            sample
            action
            \(\Action _t^j \sim \policy ^{\theta }(\bullet \mid \State _t^{j})\)
            <a id="x7-6010r7"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            obtain
            state
            from
            transition
            function
            \(\State _{t+1}^j \sim \transit (\bullet \mid \State _t^j, \Action _t^j)\)
          </span><a id="x7-6011r8"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span>
</span><a id="x7-6012r9"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x7-6013r10"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Optimize critic \(\hat \Value ^{w}\) using gradient descent to minimize the temporal difference loss \eqref{eqn_td_loss}
        <a id="x7-6014r11"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Estimate the advantages \(\hat \Advan ^{w}(\State _t, \Action _t)\) as described in \eqref{eqn_advan_est} <a id="x7-6015r12"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Update policy weights \(\theta _{i + 1} = \arg \min \) \eqref{eqn_ppo_objective_est} by performing gradient descent
      </span><a id="x7-6016r13"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x7-6017r14"></a>
<a id="x7-6018r15"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> </span>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">return </span> Optimal policy \(\policy ^{\theta _{\mathcal I}}\)
   </div>
<!-- l. 151 -->
<!-- l. 151 --><p class="indent"> <a id="tailppoli6.html"></a> </p>
</body>
</html>