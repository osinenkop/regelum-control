<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Monte Carlo estimation of the update rule</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ppo.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="monte-carlo-estimation-of-the-update-rule" link="x6-5000" md-heading="Monte Carlo estimation of the update rule" name="md-heading" type="h3"/></head><body>
<!-- l. 99 -->
<!-- l. 100 --><p class="noindent">In practice, the objective
                                         \[ \begin {aligned} &amp;\E [\transit , \policy ^{\theta _i}]{\sum _{t=0}^{\infty } \gamma ^ t \max \left (\Advan _t^{\policy ^{\theta _i}} r_t(\theta , \theta _i), \Advan _t^{\policy ^{\theta _i}}\operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (r_t(\theta , \theta _i) \right )\right )} = \\ &amp;\E [\transit , \policy ^{\theta _i}]{\sum _{t=0}^{\infty } \gamma ^ t \max \left (\Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t \mid \State _t)}, \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (\frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t \mid \State _t)}\right ) \right )} \end {aligned} \]
 can be approximated using Monte Carlo methods. Specifically, by sampling \(M\) trajectories
(referred to as <span class="cmti-12">episodes</span>), which are generated using the policy \(\pi ^{\theta _i}\), and then averaging across
these episodes, we obtain the following approximation:
                                         \[ \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \max \left (\Advan ^{\policy ^{\theta _i}}(\State _t^j, \Action _t^j) \frac {\policy ^{\theta }(\Action _t^j \mid \State _t^j)}{\policy ^{\theta _i}(\Action _t^j \mid \State _t^j)}, \Advan ^{\policy ^{\theta _i}}(\State _t^j, \Action _t^j) \operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (\frac {\policy ^{\theta }(\Action _t^j \mid \State _t^j)}{\policy ^{\theta _i}(\Action _t^j \mid \State _t^j)}\right ) \right ), \]
 where \(T\) denotes the length of an episode, and \(\State _t^j\) and \(\Action _t^j\) represent the state and action
at time \(t\) of the \(j\)-th episode, respectively. However, the advantage function \(\Advan ^{\policy ^{\theta _i}}(s, a)\) is
                                                                                   
                                                                                   
generally unknown and must be estimated. This estimation can be performed by
minimizing the temporal difference (TD) loss for the value function \(\Value ^{\policy ^{\theta _i}}\), as described
by the following equation: \begin {equation}  \label {eqn_td_loss} \sum _{j=1}^M\sum _{t=0}^{T-1 - N_{\text {TD}}}\left (\hat \Value ^w(\State _t^j) - \sum _{t'=t}^{t + N_{\text {TD}} - 1} \gamma ^{t'-t}\Cost (\State _{t'}^j, \Action _{t'}^j) - \hat \Value ^w(\State _{t + N_{\text {TD}}}^j)\right )^2 \ra \min _{w},  \end {equation}<a id="x6-5001r5"></a> where \(N_{\text {TD}}\) is the TD window, a hyperparameter. After
optimization, we achieve an approximation of the value function, \(V^{\pi _{\theta _i}} \approx \hat {V}^w\), which allows
us to estimate the advantage function as follows: \begin {equation}  \label {eqn_advan_est} \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \approx \Cost (\State _t, \Action _t) + \gamma \hat \Value ^w(\State _{t+1}) - \hat \Value ^w(\State _t) =: \hat \Advan ^{w}(\State _t, \Action _t) ,  \end {equation}<a id="x6-5002r6"></a> Substituting this estimated
advantage into the original objective yields the final PPO objective estimate:
\begin {equation}  \label {eqn_ppo_objective_est} \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \max \left (\hat \Advan ^{w}(\State _t, \Action _t) \frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t^j \mid \State _t^j)}, \hat \Advan ^{w}(\State _t^j, \Action _t^j) \operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (\frac {\policy ^{\theta }(\Action _t^j \mid \State _t^j)}{\policy ^{\theta _i}(\Action _t^j \mid \State _t^j)}\right ) \right )  \end {equation}<a id="x6-5003r7"></a>
</p>
<!-- l. 129 -->
<!-- l. 129 --><p class="indent"> <a id="tailppoli5.html"></a> </p>
</body>
</html>