{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Please ensure that the following requirements are installed prior to executing the cells within the Jupyter notebook\n"]}, {"cell_type": "code", "source": ["!pip install regelum-control"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Mastering policy gradient with Regelum. \n", "\n", "In this tutorial we will implement a well-known policy gradient algorithm [Vanilla Policy Gradient](http://regelum.aidynamic.io/tex/vpg/) with General Advantage Estimation.\n", "\n", "Moreover, we will create a complex system composed of two different kinematic systems, namely, three wheeled robot and a kinematic point.\n", "\n", "Appears that this perfectly represents an environment from a well-known problem, namely, [homicidal chauffeur problem](https://en.wikipedia.org/wiki/Homicidal_chauffeur_problem).\n", "\n", "## Vanilla Policy Gradient.\n", "\n", "Our plan is to implement:\n", "\n", "1. A pedestrian-chauffeur controlled system with circle boundary\n", "2. Policy VPG objective\n", "3. Stochastic policy\n", "    1. Stochastic model for action sampling\n", "    2. Respective Policy with optimization procedure\n", "4. Critic\n", "5. Proper scenario, running costs and other basic entities needed to construct a pipeline\n", "6. Create suitable callbacks in order to properly log a processing of our pipeline\n", "7. Analyse historical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from regelum.optimizable.core.configs import TorchOptimizerConfig\n", "from regelum.__internal.base import apply_callbacks\n", "from regelum.policy import Policy\n", "from regelum.scenario import RLScenario\n", "from regelum.data_buffers import DataBuffer\n", "from regelum.callback import (\n", "    Callback,\n", "    HistoricalCallback,\n", "    ScenarioStepLogger,\n", "    HistoricalDataCallback,\n", "    ValueCallback\n", ")\n", "from regelum.system import ComposedSystem, KinematicPoint, ThreeWheeledRobotKinematic\n", "from regelum.objective import RunningObjective\n", "from regelum.critic import Critic\n", "from regelum.model import (\n", "    PerceptronWithTruncatedNormalNoise,\n", "    ModelQuadLin,\n", "    MultiplyByConstant,\n", "    ModelNN,\n", "    ModelPerceptron\n", ")\n", "from regelum.utils import rg\n", "from regelum.simulator import CasADi\n", "from regelum.event import Event\n", "from regelum import set_ipython_env\n", "from regelum.data_buffers.batch_sampler import RollingBatchSampler\n", "from typing import List\n", "\n", "\n", "import torch as th\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import display, clear_output\n", "\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DISCOUNT_FACTOR = 0.95\n", "STATE_INIT: np.ndarray = rg.array([1.0, 1.0, 0.0, 0.0, np.pi / 4])\n", "STATE_NAMING = [\"p_x\", \"p_y\", \"c_x\", \"c_y\", \"c_theta\"]\n", "SAMPLING_TIME: float = 0.05\n", "SIM_STEP: float = SAMPLING_TIME\n", "ITERS_AGENT_SWITCH: int = 10\n", "N_EPISODES_IN_ITERATION: int = 3\n", "N_ITERATIONS: int = 200"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Define system\n", "\n", "To successfully implement our system we have to define the boundary and dynamics of systems when approaching this boundary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def circle_bound(point):\n", "    return (\n", "        point[0] ** 2 + point[1] ** 2 - 25\n", "    )  # Presume our environment is bounded with a circle of radius 5\n", "\n", "\n", "class KinematicPointRestricted(KinematicPoint):\n", "\n", "    def _compute_state_dynamics(self, time, state, inputs):\n", "        Dstate = super()._compute_state_dynamics(time, state, inputs)\n", "        return rg.if_else(\n", "            circle_bound(state) <= -1e-7,\n", "            Dstate,\n", "            rg.if_else(\n", "                rg.dot(Dstate, state) < 0,\n", "                Dstate,\n", "                rg.zeros(2, prototype=Dstate),\n", "            ),\n", "        )  # When approaching boundary, only can move in directions to the circle interior\n", "\n", "\n", "class ThreeWheeledRobotKinematicRestricted(ThreeWheeledRobotKinematic):\n", "    def _compute_state_dynamics(self, time, state, inputs):\n", "        Dstate = super()._compute_state_dynamics(time, state, inputs)\n", "        return rg.if_else(\n", "            circle_bound(state[:2]) <= -1e-7,\n", "            Dstate,\n", "            rg.if_else(\n", "                rg.dot(Dstate[:2], state[:2]) < 0,\n", "                Dstate,\n", "                rg.zeros(3, prototype=Dstate),\n", "            ),\n", "        )  # When approaching boundary, only can move in directions to the circle interior\n", "\n", "\n", "homicidal_chauffeur_system = ComposedSystem(\n", "    sys_left=KinematicPointRestricted(),\n", "    sys_right=ThreeWheeledRobotKinematicRestricted(),\n", "    output_mode=\"both\",\n", "    io_mapping=[],  # means that no output of the left system is used as input to the right one.\n", "    action_bounds=KinematicPoint._action_bounds\n", "    + ThreeWheeledRobotKinematic._action_bounds,\n", "    state_naming=STATE_NAMING,\n", ")  # This concatenated system perfectly suits for homicidal chauffeur modelling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["KinematicPointRestricted().compute_state_dynamics(\n", "    None, rg.array([1, 2]), rg.array([1, 1])\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ThreeWheeledRobotKinematicRestricted().compute_state_dynamics(\n", "    None,\n", "    rg.array([0, 0, np.pi / 4], rc_type=rg.CASADI),\n", "    rg.array([1, 0], rc_type=rg.CASADI),\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["homicidal_chauffeur_system.compute_state_dynamics(\n", "    None,\n", "    rg.array([0, 0, 0, 0, np.pi / 4], rc_type=rg.CASADI),\n", "    rg.array([1, 1, 1, 0], rc_type=rg.CASADI),\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["homicidal_chauffeur_system.compute_state_dynamics(\n", "    None, rg.array_symb((1, 5), literal=\"s\"), rg.array_symb((1, 4), literal=\"u\")\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Policy VPG objective\n", "\n", "To simplify an implementation we will split it into two parts:\n", "\n", "- GAE computation\n", "- VPG objective"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_gae_advantage(\n", "    gae_lambda: float,\n", "    running_objectives: th.FloatTensor,\n", "    values: th.FloatTensor,\n", "    times: th.FloatTensor,\n", "    discount_factor: float,\n", "    sampling_time: float,\n", ") -> th.FloatTensor:\n", "    deltas = (\n", "        running_objectives[:-1]\n", "        + discount_factor**sampling_time * values[1:]\n", "        - values[:-1]\n", "    )\n", "    if gae_lambda == 0.0:\n", "        advantages = deltas\n", "    else:\n", "        gae_discount_factors = (gae_lambda * discount_factor) ** times[:-1]\n", "        reversed_gae_discounted_deltas = th.flip(\n", "            gae_discount_factors * deltas, dims=[0, 1]\n", "        )\n", "        advantages = (\n", "            th.flip(reversed_gae_discounted_deltas.cumsum(dim=0), dims=[0, 1])\n", "            / gae_discount_factors\n", "        )\n", "    return advantages\n", "\n", "\n", "def vpg_objective(\n", "    policy_model: PerceptronWithTruncatedNormalNoise,\n", "    critic_model: ModelNN,\n", "    observations: th.FloatTensor,\n", "    actions: th.FloatTensor,\n", "    times: th.FloatTensor,\n", "    episode_ids: th.LongTensor,\n", "    discount_factor: float,\n", "    N_episodes: int,\n", "    running_objectives: th.FloatTensor,\n", "    sampling_time: float,\n", "    is_normalize_advantages: bool,\n", "    gae_lambda: float,\n", ") -> th.FloatTensor:\n", "    critic_values = critic_model(observations)\n", "    log_pdfs = policy_model.log_pdf(observations, actions).reshape(-1)\n", "\n", "    objective = 0.0\n", "    for episode_idx in th.unique(episode_ids):\n", "        mask = (episode_ids == episode_idx).reshape(-1)\n", "        advantages = get_gae_advantage(\n", "            gae_lambda=gae_lambda,\n", "            running_objectives=running_objectives[mask],\n", "            values=critic_values[mask],\n", "            times=times[mask],\n", "            discount_factor=discount_factor,\n", "            sampling_time=sampling_time,\n", "        ).reshape(-1)\n", "\n", "        if is_normalize_advantages:\n", "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n", "\n", "        objective += (\n", "            discount_factor ** times[mask][:-1].reshape(-1)\n", "            * advantages\n", "            * log_pdfs[mask][:-1]\n", "        ).sum()\n", "\n", "    return objective / N_episodes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Stochastic double-model policy\n", "\n", "Now, we define the stochastic policy with two models that should act and be optimized separately.\n", "\n", "We will achieve it passing two separate instances of `PerceptronWithTruncatedNormalNoise` and two respective critics into it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class JointPolicyVPG(Policy):\n", "    def __init__(\n", "        self,\n", "        pedestrian_model: PerceptronWithTruncatedNormalNoise,\n", "        chauffeur_model: PerceptronWithTruncatedNormalNoise,\n", "        pedestrian_critic: Critic,\n", "        chauffeur_critic: Critic,\n", "        system: ComposedSystem,\n", "        is_normalize_advantages: bool = True,\n", "        gae_lambda: float = 0.95,\n", "        N_episodes: int = 1,\n", "        sampling_time: float = 0.1,\n", "    ):\n", "        def freeze_stds(params):\n", "            for p in params():\n", "                if p[0] == \"stds\":\n", "                    p[1].requires_grad_(False)\n", "            return params  # Detaches parameters named \"stds\"\n", "\n", "        iter_batches_kwargs = {\n", "            \"batch_sampler\": RollingBatchSampler,\n", "            \"dtype\": th.FloatTensor,\n", "            \"mode\": \"full\",\n", "            \"n_batches\": 1,\n", "            \"device\": \"cpu\",\n", "        }\n", "        super().__init__(\n", "            system=system,\n", "            optimizer_config=TorchOptimizerConfig(\n", "                n_epochs=1,  # Only one grad step\n", "                data_buffer_iter_bathes_kwargs=iter_batches_kwargs,\n", "                opt_method_kwargs=dict(lr=1e-3),\n", "            ),\n", "            action_bounds=system.sys_left.action_bounds\n", "            + system.sys_right.action_bounds,\n", "        )\n", "        self.is_normalize_advantages = is_normalize_advantages\n", "        self.gae_lambda = gae_lambda\n", "        self.pedestrian_model = pedestrian_model\n", "        self.chauffeur_model = chauffeur_model\n", "        self.model_to_optimize = self.pedestrian_model\n", "\n", "        self.pedestrian_critic = pedestrian_critic\n", "        self.chauffeur_critic = chauffeur_critic\n", "        self.current_critic = self.pedestrian_critic\n", "        self.N_episodes = N_episodes\n", "        self.sampling_time = sampling_time\n", "\n", "        ## Define an optimization problem here\n", "\n", "        self.pedestrian_model_weigths = self.create_variable(\n", "            name=\"pedestrian_model_weights\", like=self.pedestrian_model.named_parameters\n", "        )\n", "        self.pedestrian_model_weigths.register_hook(freeze_stds)\n", "\n", "        self.chauffeur_model_weights = self.create_variable(\n", "            name=\"chauffeur_model_weights\",\n", "            like=self.chauffeur_model.named_parameters,\n", "            is_constant=True,\n", "        )\n", "        self.chauffeur_model_weights.register_hook(freeze_stds)\n", "\n", "        self.objective_inputs = [\n", "            self.create_variable(name=variable_name, is_constant=True)\n", "            for variable_name in self.data_buffer_objective_keys()\n", "        ]\n", "        self.register_objective(\n", "            self.objective_function, variables=self.objective_inputs\n", "        )\n", "\n", "    def switch_critic(self):\n", "        # This method will be triggered from Scenario\n", "        self.current_critic = (\n", "            self.pedestrian_critic\n", "            if self.current_critic is self.chauffeur_critic\n", "            else self.chauffeur_critic\n", "        )\n", "\n", "    def switch_model_to_optimize(self):\n", "        # This method will be triggered from Scenario\n", "        self.model_to_optimize = (\n", "            self.pedestrian_model\n", "            if self.model_to_optimize is self.chauffeur_model\n", "            else self.chauffeur_model\n", "        )\n", "        return self.model_to_optimize\n", "\n", "    def action_col_idx(self):\n", "        return (\n", "            slice(0, self.system.sys_left.dim_inputs)\n", "            if self.model_to_optimize is self.pedestrian_model\n", "            else slice(self.system.sys_right.dim_inputs, None)\n", "        )\n", "\n", "    def objective_function(\n", "        self,\n", "        observation: th.Tensor,  # All arguments here will be passed from data buffer\n", "        action: th.Tensor,\n", "        time: th.Tensor,\n", "        episode_id: th.Tensor,\n", "        running_objective: th.Tensor,\n", "    ):\n", "        actions_of_current_model = action[\n", "            :, self.action_col_idx()\n", "        ]  # Choose respective actions\n", "        return vpg_objective(\n", "            policy_model=self.model_to_optimize,\n", "            critic_model=self.current_critic.model,\n", "            observations=observation,\n", "            actions=actions_of_current_model,\n", "            times=time,\n", "            discount_factor=self.discount_factor,\n", "            N_episodes=self.N_episodes,\n", "            episode_ids=episode_id.long(),\n", "            running_objectives=running_objective,\n", "            sampling_time=self.sampling_time,\n", "            is_normalize_advantages=self.is_normalize_advantages,\n", "            gae_lambda=self.gae_lambda,\n", "        )\n", "\n", "    def get_action(self, observation: np.array) -> np.array:\n", "        action_pedestrian = self.pedestrian_model(th.FloatTensor(observation))\n", "        action_chauffeur = self.chauffeur_model(th.FloatTensor(observation))\n", "        action = rg.hstack((action_pedestrian, action_chauffeur)).detach().cpu().numpy()\n", "        return action  # Concatenate actions in order to pass them as a whole into Scenario at runtime\n", "\n", "    def data_buffer_objective_keys(self):\n", "        return [\"observation\", \"action\", \"time\", \"episode_id\", \"running_objective\"]\n", "\n", "    @apply_callbacks()\n", "    def optimize(self, data_buffer: DataBuffer) -> None:\n", "        opt_kwargs = data_buffer.get_optimization_kwargs(\n", "            keys=self.data_buffer_objective_keys(),\n", "            optimizer_config=self.optimizer_config,\n", "        )\n", "        super().optimize_tensor(**opt_kwargs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Critic\n", "\n", "As we will use a common Value critic, we will just instantiate two of them responsible for the pedestrian and chauffeur agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pedestrian_critic_model = ModelPerceptron(\n", "    dim_input=5,\n", "    dim_output=1,\n", "    dim_hidden=40,\n", "    n_hidden_layers=3,\n", "    hidden_activation=th.nn.Tanh(),\n", ")\n", "\n", "chauffeur_critic_model = ModelPerceptron(\n", "    dim_input=5,\n", "    dim_output=1,\n", "    dim_hidden=40,\n", "    n_hidden_layers=3,\n", "    hidden_activation=th.nn.Tanh(),\n", ")\n", "\n", "iter_batches_kwargs = {\n", "    \"batch_sampler\": RollingBatchSampler,\n", "    \"dtype\": th.FloatTensor,\n", "    \"mode\": \"full\",\n", "    \"n_batches\": 1,\n", "    \"device\": \"cpu\",\n", "}\n", "\n", "pedestrian_critic_optimizer_config = TorchOptimizerConfig(\n", "    n_epochs=20,\n", "    data_buffer_iter_bathes_kwargs=iter_batches_kwargs,\n", "    opt_method_kwargs=dict(lr=1e-3),\n", ")\n", "\n", "chauffeur_critic_optimizer_config = TorchOptimizerConfig(\n", "    n_epochs=20,\n", "    data_buffer_iter_bathes_kwargs=iter_batches_kwargs,\n", "    opt_method_kwargs=dict(lr=1e-3),\n", ")\n", "\n", "pedestrian_critic = Critic(\n", "    system=homicidal_chauffeur_system.sys_left,\n", "    model=pedestrian_critic_model,\n", "    td_n=2,\n", "    is_value_function=True,\n", "    is_on_policy=True,\n", "    optimizer_config=pedestrian_critic_optimizer_config,\n", "    discount_factor=DISCOUNT_FACTOR,\n", ")\n", "\n", "chauffeur_critic = Critic(\n", "    system=homicidal_chauffeur_system.sys_right,\n", "    model=chauffeur_critic_model,\n", "    td_n=2,\n", "    is_value_function=True,\n", "    is_on_policy=True,\n", "    optimizer_config=chauffeur_critic_optimizer_config,\n", "    discount_factor=DISCOUNT_FACTOR,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PedestrianRunningObjectiveModel(ModelQuadLin):\n", "    def __init__(self, weights=rg.array([10, 0, 0, 0, 0])):\n", "        super().__init__(\n", "            quad_matrix_type=\"diagonal\",\n", "            dim_inputs=4,\n", "            weights=weights,\n", "        )\n", "        self.eps = 1.0e-5\n", "\n", "    def __call__(self, *argin, **kwargs):\n", "        observation = argin[0]\n", "        pedestrian_pos = observation[0, :2]\n", "        chauffeur_pos = observation[0, 2:-1]\n", "        distance = rg.norm_2(pedestrian_pos - chauffeur_pos)  # square norm of distance\n", "        inv_distance = rg.array(\n", "            [\n", "                [(1 / (distance + self.eps)) if distance < 2 else 0]\n", "            ]  # We penalize a pedestrian for close distance to the chauffeur\n", "        )\n", "        action = argin[1]\n", "        return super().__call__(inv_distance, action, **kwargs)\n", "\n", "\n", "class ChauffeurRunningObjectiveModel(ModelQuadLin):\n", "    def __init__(self, weights=rg.array([10, 0, 0, 0, 0])):\n", "        super().__init__(\n", "            quad_matrix_type=\"diagonal\",\n", "            dim_inputs=4,\n", "            weights=weights,\n", "        )\n", "\n", "    def __call__(self, *argin, **kwargs):\n", "        observation = argin[0]\n", "        pedestrian_pos = observation[0, :2]\n", "        chauffeur_pos = observation[0, 2:-1]\n", "        distance = rg.array([[rg.norm_2(pedestrian_pos - chauffeur_pos)]])\n", "        action = argin[1]\n", "        return super().__call__(distance, action, **kwargs)\n", "\n", "\n", "simulator = CasADi(\n", "    system=homicidal_chauffeur_system,\n", "    state_init=STATE_INIT,\n", "    time_final=7,\n", "    max_step=SIM_STEP,\n", ")\n", "\n", "\n", "class GameScenario(RLScenario):\n", "    def __init__(\n", "        self,\n", "        policy: JointPolicyVPG,\n", "        pedestrian_critic: Critic,\n", "        chauffeur_critic: Critic,\n", "        simulator: CasADi,\n", "        pedestrian_running_objective_model: PedestrianRunningObjectiveModel,\n", "        chauffeur_running_objective_model: ChauffeurRunningObjectiveModel,\n", "        discount_factor: float = DISCOUNT_FACTOR,\n", "        sampling_time: float = SAMPLING_TIME,\n", "        N_iterations: int = 200,\n", "        is_parallel: bool = True,\n", "        iters_to_switch_opt_agent: int = 1,\n", "    ):\n", "        self.pedestrian_running_objective = RunningObjective(\n", "            model=pedestrian_running_objective_model\n", "        )\n", "        self.chauffeur_running_objective = RunningObjective(\n", "            model=chauffeur_running_objective_model\n", "        )\n", "        self.pedestrian_critic = pedestrian_critic\n", "        self.chauffeur_critic = chauffeur_critic\n", "\n", "        self.iters_to_switch_opt_agent = iters_to_switch_opt_agent\n", "        super().__init__(\n", "            policy=policy,\n", "            critic=pedestrian_critic,\n", "            running_objective=self.pedestrian_running_objective,\n", "            simulator=simulator,\n", "            policy_optimization_event=Event.reset_iteration,\n", "            discount_factor=discount_factor,\n", "            sampling_time=sampling_time,\n", "            N_episodes=policy.N_episodes,\n", "            N_iterations=N_iterations,\n", "            is_parallel=is_parallel,\n", "            is_critic_first=True,\n", "        )\n", "        self.policy: JointPolicyVPG\n", "\n", "    def switch_running_objective(self):\n", "        self.running_objective = (\n", "            self.pedestrian_running_objective\n", "            if self.running_objective is self.chauffeur_running_objective\n", "            else self.chauffeur_running_objective\n", "        )\n", "\n", "    def switch_critic(self):\n", "        self.critic = (\n", "            self.pedestrian_critic\n", "            if self.critic is self.chauffeur_critic\n", "            else self.chauffeur_critic\n", "        )\n", "\n", "    @apply_callbacks()\n", "    def compute_action_sampled(self, time, estimated_state, observation):\n", "        return super().compute_action_sampled(time, estimated_state, observation)\n", "\n", "    @apply_callbacks()  # We will add a callbacks to log it when we switch an optimizing agent\n", "    def switch_optimizing_agent(self):\n", "        self.switch_running_objective()\n", "        policy_weights_to_fix, policy_weights_to_unfix = (\n", "            [\"pedestrian_model_weights\", \"chauffeur_model_weights\"]\n", "            if self.running_objective.model is self.chauffeur_running_objective.model\n", "            else [\"chauffeur_model_weights\", \"pedestrian_model_weights\"]\n", "        )\n", "        self.policy.fix_variables([policy_weights_to_fix])\n", "        self.policy.unfix_variables([policy_weights_to_unfix])\n", "        self.policy.switch_model_to_optimize()\n", "        self.policy.switch_critic()\n", "        self.switch_critic()\n", "        return policy_weights_to_fix, policy_weights_to_unfix\n", "\n", "    def reset_iteration(self):\n", "        super().reset_iteration()\n", "        if self.iteration_counter % self.iters_to_switch_opt_agent == 0:\n", "            self.switch_optimizing_agent()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pedestrian_model = PerceptronWithTruncatedNormalNoise(\n", "    dim_input=5,\n", "    dim_output=2,\n", "    dim_hidden=40,\n", "    n_hidden_layers=3,\n", "    hidden_activation=th.nn.Tanh(),\n", "    output_bounds=homicidal_chauffeur_system.sys_left.action_bounds,\n", "    is_truncated_to_output_bounds=True,\n", "    output_activation=MultiplyByConstant(0.1),\n", "    stds=[0.05] * 2,\n", ")\n", "\n", "chauffeur_model = PerceptronWithTruncatedNormalNoise(\n", "    dim_input=5,\n", "    dim_output=2,\n", "    dim_hidden=40,\n", "    n_hidden_layers=3,\n", "    hidden_activation=th.nn.Tanh(),\n", "    output_bounds=homicidal_chauffeur_system.sys_left.action_bounds,\n", "    is_truncated_to_output_bounds=True,\n", "    output_activation=MultiplyByConstant(0.1),\n", "    stds=[0.05] * 2,\n", ")\n", "\n", "\n", "policy = JointPolicyVPG(\n", "    pedestrian_model=pedestrian_model,\n", "    chauffeur_model=chauffeur_model,\n", "    pedestrian_critic=pedestrian_critic,\n", "    chauffeur_critic=chauffeur_critic,\n", "    system=homicidal_chauffeur_system,\n", "    is_normalize_advantages=True,\n", "    gae_lambda=0.95,\n", "    N_episodes=N_EPISODES_IN_ITERATION,\n", "    sampling_time=SAMPLING_TIME,\n", ")\n", "\n", "scenario = GameScenario(\n", "    policy=policy,\n", "    pedestrian_critic=pedestrian_critic,\n", "    chauffeur_critic=chauffeur_critic,\n", "    simulator=simulator,\n", "    pedestrian_running_objective_model=PedestrianRunningObjectiveModel(),\n", "    chauffeur_running_objective_model=ChauffeurRunningObjectiveModel(),\n", "    discount_factor=DISCOUNT_FACTOR,\n", "    sampling_time=SAMPLING_TIME,\n", "    is_parallel=False,  # Logging is not working in parallel mode while in jupyter, so we disable it for now.\n", "    N_iterations=N_ITERATIONS,\n", "    iters_to_switch_opt_agent=ITERS_AGENT_SWITCH,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implement callbacks\n", "\n", "Instead of printing every step we doubt, we can implement callbacks logging and saving interim data for us.\n", "\n", "In this particular situation we may be interested in the following:\n", "\n", "- Whether the switching between agents performs correctly\n", "- How do the values of our agents evolve over time\n", "- Which object is optimizing currently\n", "\n", "Let's implement callbacks which will log these data for us."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SwitchAgentCallback(Callback):\n", "    def is_target_event(self, obj, method, output, triggers):\n", "        return method == \"switch_optimizing_agent\"\n", "\n", "    def on_function_call(self, obj: GameScenario, method, output):\n", "        clear_output()\n", "        self.log(\n", "            \"Optimizing agent switched to\"\n", "            f\" {'pedestrian' if output[1] == 'pedestrian_model_weights' else 'chauffeur'} \"\n", "            f\"after iterartion: {obj.iteration_counter}\"\n", "        )\n", "\n", "\n", "class DoubleAgentStepLogger(HistoricalCallback):\n", "    def __init__(self, *args, **kwargs):\n", "        super().__init__(*args, **kwargs)\n", "        self.cooldown = 0.0\n", "        self.state_components_naming = STATE_NAMING\n", "\n", "    def is_target_event(self, obj, method, output, triggers):\n", "        return (method == \"post_compute_action\") and isinstance(obj, GameScenario)\n", "\n", "    def on_function_call(self, obj: GameScenario, method, output):\n", "        self.add_datum(\n", "            {\n", "                **{\n", "                    \"time\": output[\"time\"],\n", "                    \"running_objective_pedestrian\": obj.pedestrian_running_objective(\n", "                        output[\"estimated_state\"], output[\"action\"]\n", "                    ),\n", "                    \"running_objective_chauffeur\": obj.chauffeur_running_objective(\n", "                        output[\"estimated_state\"], output[\"action\"]\n", "                    ),\n", "                    \"episode_id\": output[\"episode_id\"],\n", "                    \"iteration_id\": output[\"iteration_id\"],\n", "                },\n", "                **dict(zip(self.state_components_naming, output[\"estimated_state\"][0])),\n", "            }\n", "        )\n", "\n", "\n", "class WhichOptimizeCallback(Callback):\n", "    def is_target_event(self, obj, method, output, triggers):\n", "        return isinstance(obj, JointPolicyVPG) and method == \"optimize\"\n", "\n", "    def on_function_call(self, obj: JointPolicyVPG, method, output):\n", "        which = (\n", "            \"pedestrian\"\n", "            if obj.chauffeur_model_weights.is_constant\n", "            else \"chauffeur_model\"\n", "        )\n", "        self.log(f\"A {which} policy updated...\")\n", "\n", "\n", "callbacks = [SwitchAgentCallback, DoubleAgentStepLogger, WhichOptimizeCallback]\n", "callbacks = set_ipython_env(callbacks=callbacks, interactive=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run it!"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/html": ["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:24:18] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> A pedestrian policy updated<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                 <a href=\"file:///tmp/ipykernel_1441066/2327856449.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2327856449.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1441066/2327856449.py#52\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52</span></a>\n", "</pre>\n"], "text/plain": ["\u001b[2;36m[15:24:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m A pedestrian policy updated\u001b[33m...\u001b[0m                                                 \u001b]8;id=237117;file:///tmp/ipykernel_1441066/2327856449.py\u001b\\\u001b[2m2327856449.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=121522;file:///tmp/ipykernel_1441066/2327856449.py#52\u001b\\\u001b[2m52\u001b[0m\u001b]8;;\u001b\\\n"]}, "metadata": {}, "output_type": "display_data"}], "source": ["scenario.run()  # Takes approximately 6-7 minutes to finish 200 iterations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Analysis of historical data collected"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_trajectories(callback, episode_id, iteration_id):\n", "    data = callback.data[\n", "        (callback.data.episode_id == episode_id)\n", "        & (callback.data.iteration_id == iteration_id)\n", "    ][homicidal_chauffeur_system.state_naming[:-1]]\n", "    pedestrian_data = data[homicidal_chauffeur_system.state_naming[:2]]\n", "    chauffeur_data = data[homicidal_chauffeur_system.state_naming[2:-1]]\n", "    plt.scatter(pedestrian_data[\"p_x\"], pedestrian_data[\"p_y\"])\n", "    plt.scatter(chauffeur_data[\"c_x\"], chauffeur_data[\"c_y\"])\n", "    plt.scatter(\n", "        chauffeur_data[\"c_x\"].iloc[-1],\n", "        chauffeur_data[\"c_y\"].iloc[-1],\n", "        c=\"r\",\n", "        label=\"chauffeur final\",\n", "    )\n", "    plt.scatter(\n", "        pedestrian_data[\"p_x\"].iloc[-1],\n", "        pedestrian_data[\"p_y\"].iloc[-1],\n", "        c=\"g\",\n", "        label=\"pedestrian final\",\n", "    )\n", "    plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_trajectories(callback=callbacks[1], episode_id=1, iteration_id=50)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_trajectories(callback=callbacks[1], episode_id=1, iteration_id=200)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history = callbacks[1].data\n", "history[\"value_pedestrian\"] = history.groupby([\"iteration_id\", \"episode_id\"])[\n", "    [\"running_objective_pedestrian\"]\n", "].cumsum()\n", "\n", "mean_iter_values = history.groupby([\"iteration_id\"])[\"value_pedestrian\"].mean()\n", "mean_iter_values.plot(figsize=(10, 5), label=\"pedestrian learning curve\")\n", "\n", "\n", "for x in range(0, len(mean_iter_values), ITERS_AGENT_SWITCH * 2):\n", "    plt.axvline(x=x, c=\"r\")  # Plot moments of switching between agents being optimized\n", "\n", "plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history[\"value_chauffeur\"] = history.groupby([\"iteration_id\", \"episode_id\"])[\n", "    [\"running_objective_chauffeur\"]\n", "].cumsum()\n", "\n", "mean_iter_values = history.groupby([\"iteration_id\"])[\"value_chauffeur\"].mean()\n", "mean_iter_values.plot(figsize=(10, 5), label=\"chauffeur learnign curve\")\n", "\n", "\n", "for x in range(ITERS_AGENT_SWITCH, len(mean_iter_values), ITERS_AGENT_SWITCH * 2):\n", "    plt.axvline(x=x, c=\"r\")  # Plot moments of switching between agents being optimized\n", "\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##"]}], "metadata": {"kernelspec": {"display_name": "rgenv-dev", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}, "nbformat": 4, "nbformat_minor": 2}