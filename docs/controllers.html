<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>REINFORCE Algorithm &mdash; regelum 0.2.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
      <link rel="stylesheet" href="_static/fonts.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
      <link rel="stylesheet" href="_static/fonts.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tutorial: Implementing Custom Systems in Regelum" href="systems_tutorial.html" />
    <link rel="prev" title="Configs and basic regelum applications" href="tutorials.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home" style="color: #8cc770;">
            regelum
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#what-is-regelum">What is <code class="docutils literal notranslate"><span class="pre">regelum</span></code>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#regelum-s-config-pipeline"><code class="docutils literal notranslate"><span class="pre">regelum</span></code>’s config pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#presets">Presets</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#related-literature">Related literature</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#closing-remarks">Closing remarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#for-developers">For developers</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Configs and basic <code class="docutils literal notranslate"><span class="pre">regelum</span></code> applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html#callbacks-and-logging">Callbacks and Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html#what-if-i-still-don-t-know-what-i-m-doing">What if I still don’t know what I’m doing?</a></li>
</ul>
<p class="caption"><span class="caption-text">Controllers</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">REINFORCE Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-update-formulas">Gradient Update Formulas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#for-rewards">For Rewards</a></li>
<li class="toctree-l3"><a class="reference internal" href="#for-costs">For Costs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-steps">Algorithm Steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#notation">Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage-example-for-costs">Usage Example (for costs)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#proximal-policy-optimization-ppo-algorithm">Proximal Policy Optimization (PPO) Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-optimization-formulas">Gradient Optimization Formulas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#for-policy-updates">For Policy Updates</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Algorithm Steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">Notation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#proximal-policy-optimization-algorithm">Proximal Policy Optimization Algorithm</a></li>
</ul>
<p class="caption"><span class="caption-text">Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="systems_tutorial.html">Tutorial: Implementing Custom Systems in Regelum</a></li>
<li class="toctree-l1"><a class="reference internal" href="systems.html">Avaliable systems</a></li>
</ul>
<p class="caption"><span class="caption-text">Optimizable</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimizable.html">Optimizable Subpackage Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">rc singletone</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rc.html">The <strong>rc</strong> Singleton Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.animation.html"><code class="docutils literal notranslate"><span class="pre">regelum.animation</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.callback.html"><code class="docutils literal notranslate"><span class="pre">regelum.callback</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.configure.html"><code class="docutils literal notranslate"><span class="pre">regelum.configure</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.constraint_parser.html"><code class="docutils literal notranslate"><span class="pre">regelum.constraint_parser</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.controller.html"><code class="docutils literal notranslate"><span class="pre">regelum.controller</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.critic.html"><code class="docutils literal notranslate"><span class="pre">regelum.critic</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.data_buffers.html"><code class="docutils literal notranslate"><span class="pre">regelum.data_buffers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.data_buffers.batch_sampler.html"><code class="docutils literal notranslate"><span class="pre">regelum.data_buffers.batch_sampler</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.data_buffers.data_buffer.html"><code class="docutils literal notranslate"><span class="pre">regelum.data_buffers.data_buffer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.data_buffers.fifo_list.html"><code class="docutils literal notranslate"><span class="pre">regelum.data_buffers.fifo_list</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.data_buffers.types.html"><code class="docutils literal notranslate"><span class="pre">regelum.data_buffers.types</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.event.html"><code class="docutils literal notranslate"><span class="pre">regelum.event</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.model.html"><code class="docutils literal notranslate"><span class="pre">regelum.model</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.objective.html"><code class="docutils literal notranslate"><span class="pre">regelum.objective</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.observer.html"><code class="docutils literal notranslate"><span class="pre">regelum.observer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.core.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable.core</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.core.configs.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable.core.configs</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.core.entities.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable.core.entities</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.core.hooks.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable.core.hooks</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.optimizable.optimizers.html"><code class="docutils literal notranslate"><span class="pre">regelum.optimizable.optimizers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.policy.html"><code class="docutils literal notranslate"><span class="pre">regelum.policy</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.predictor.html"><code class="docutils literal notranslate"><span class="pre">regelum.predictor</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.simulator.html"><code class="docutils literal notranslate"><span class="pre">regelum.simulator</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/regelum.system.html"><code class="docutils literal notranslate"><span class="pre">regelum.system</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">regelum</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" style="color: #8cc770;" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">REINFORCE Algorithm</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reinforce-algorithm">
<h1>REINFORCE Algorithm<a class="headerlink" href="#reinforce-algorithm" title="Permalink to this headline"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The <strong>ReinforceController</strong> implements the <strong>REINFORCE</strong> algorithm, which is a Monte Carlo policy gradient method used in reinforcement learning. This method seeks to optimize the policy by updating parameters in the direction that maximizes expected rewards (or minimizes the expected costs).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We provide functionality for policy actions to be generated from a truncated normal distribution that is truncated to the action bounds provided in the system as default parameters.</p>
</div>
</section>
<section id="gradient-update-formulas">
<h2>Gradient Update Formulas<a class="headerlink" href="#gradient-update-formulas" title="Permalink to this headline"></a></h2>
<p>Let us denote <span class="math notranslate nohighlight">\(\theta_i\)</span> as policy weights at <span class="math notranslate nohighlight">\(i\)</span>-th iteration, <span class="math notranslate nohighlight">\(B_k\)</span> as the baseline — random variable that is independent
of action <span class="math notranslate nohighlight">\(U_k\)</span> conditioned on observation <span class="math notranslate nohighlight">\(Y_k\)</span>.</p>
<section id="for-rewards">
<h3>For Rewards<a class="headerlink" href="#for-rewards" title="Permalink to this headline"></a></h3>
<p>When optimizing for rewards, the general formula for the gradient update is:</p>
<div class="math notranslate nohighlight">
\[\theta_{i+1} \leftarrow \theta_{i} + \alpha \mathbb{E}\left[ \sum_{k = 0}^{N-1} \left( \sum_{k'=k}^{N-1} \gamma^{k'}r(Y_{k'}, U_{k'}) - B_k \right)
\nabla_{\theta} \ln\rho^{\theta}(U_k \mid Y_k)\big|_{\theta=\theta^i}\right].\]</div>
</section>
<section id="for-costs">
<h3>For Costs<a class="headerlink" href="#for-costs" title="Permalink to this headline"></a></h3>
<p>When optimizing for costs, the gradient update formula includes a negative sign:</p>
<div class="math notranslate nohighlight">
\[\theta_{i+1} \leftarrow \theta_{i} - \alpha \mathbb{E}\left[ \sum_{k = 0}^{N-1} \left( \sum_{k'=k}^{N-1} \gamma^{k'}c(Y_{k'}, U_{k'}) - B_k \right)
\nabla_{\theta} \ln\rho^{\theta}(U_k \mid Y_k)\big|_{\theta=\theta^i}\right].\]</div>
</section>
</section>
<section id="algorithm-steps">
<h2>Algorithm Steps<a class="headerlink" href="#algorithm-steps" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Initialize the baseline <span class="math notranslate nohighlight">\(B^1 = 0\)</span>.</p></li>
<li><p>For each iteration <span class="math notranslate nohighlight">\(i\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots, N\_iterations\}\)</span>:</p>
<ol class="loweralpha">
<li><p>For each episode <span class="math notranslate nohighlight">\(j\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots, M\}\)</span>:</p>
<ol class="lowerroman simple">
<li><p>For each time step <span class="math notranslate nohighlight">\(k\)</span> in <span class="math notranslate nohighlight">\(\{0, \ldots, N-1\}\)</span>:</p>
<ul class="simple">
<li><p>Obtain observation <span class="math notranslate nohighlight">\(y_k^j\)</span> from the environment.</p></li>
<li><p>Sample action <span class="math notranslate nohighlight">\(u_k^j\)</span> from a truncated normal distribution that is truncated to action bounds (provided as default parameters), using the policy <span class="math notranslate nohighlight">\(\rho^{\theta}(u_k^j | y_k^j)\)</span>.</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Compute the baseline for the next iteration:</p>
<div class="math notranslate nohighlight">
\[b^{i + 1}_k \leftarrow \frac{1}{M} \sum_{j=1}^M \sum_{k'=k}^{N-1} \gamma^{k'} r(y_{k'}^j, u_{k'}^j)\]</div>
</li>
<li><p>Perform a gradient step:</p>
<div class="math notranslate nohighlight">
\[\theta_{i+1} \leftarrow \theta_i - \alpha \frac{1}{M} \sum_{j=1}^{M} \sum_{k=0}^{N-1} \left(\sum_{k'=k}^{N-1} \gamma^{k'}
c(y_{k'}^j, u_{k'}^j) - b^i_k\right)\nabla_\theta \ln\rho^\theta(u_k^j | y_k^j)\big|_{\theta=\theta^i}\]</div>
</li>
</ol>
</li>
</ol>
</section>
<section id="notation">
<h2>Notation<a class="headerlink" href="#notation" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\_iterations\)</span>: Total number of iterations.</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span>: Number of episodes per iteration.</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span>: Number of steps per episode.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>: Discount factor for future rewards.</p></li>
<li><p><span class="math notranslate nohighlight">\(r(y_k^j, u_k^j)\)</span>: Reward received after taking action <span class="math notranslate nohighlight">\(u_k^j\)</span> in state <span class="math notranslate nohighlight">\(y_k^j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(c(Y_{k'}, U_{k'})\)</span>: Cost incurred after taking action <span class="math notranslate nohighlight">\(U_{k'}\)</span> in state <span class="math notranslate nohighlight">\(Y_{k'}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_\theta\)</span>: Gradient with respect to policy parameters.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho^\theta(u_k^j | y_k^j)\)</span>: Probability density function of the truncated normal distribution, representing the likelihood of taking action <span class="math notranslate nohighlight">\(u_k^j\)</span> in state <span class="math notranslate nohighlight">\(y_k^j\)</span> under the current policy parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</section>
<section id="usage-example-for-costs">
<h2>Usage Example (for costs)<a class="headerlink" href="#usage-example-for-costs" title="Permalink to this headline"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">regelum</span> <span class="k">as</span> <span class="nn">rg</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">regelum.system</span> <span class="kn">import</span> <span class="n">InvertedPendulumPD</span>
<span class="kn">from</span> <span class="nn">regelum.objective</span> <span class="kn">import</span> <span class="n">RunningObjective</span>
<span class="kn">from</span> <span class="nn">regelum.simulator</span> <span class="kn">import</span> <span class="n">CasADi</span>
<span class="kn">from</span> <span class="nn">regelum.controller</span> <span class="kn">import</span> <span class="n">ReinforceController</span>

<span class="c1"># Initialize the system and running cost</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">InvertedPendulumPD</span><span class="p">()</span>
<span class="n">running_cost</span> <span class="o">=</span> <span class="n">RunningObjective</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModelQuadLin</span><span class="p">(</span><span class="s2">&quot;diagonal&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># Instantiate the ReinforceController</span>
<span class="n">controller</span> <span class="o">=</span> <span class="n">ReinforceController</span><span class="p">(</span>
    <span class="n">policy_model</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">PerceptronWithTruncatedNormalNoise</span><span class="p">(</span>
        <span class="n">dim_input</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">dim_observation</span><span class="p">,</span>
        <span class="n">dim_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">dim_output</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">dim_inputs</span><span class="p">,</span>
        <span class="n">hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">output_activation</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">MultiplyByConstant</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">100</span><span class="p">),</span>
        <span class="n">output_bounds</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">action_bounds</span><span class="p">,</span>
        <span class="n">stds</span><span class="o">=</span><span class="p">[</span><span class="mf">2.5</span><span class="p">],</span>
        <span class="n">is_truncated_to_output_bounds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">simulator</span><span class="o">=</span><span class="n">CasADi</span><span class="p">(</span>
        <span class="n">system</span><span class="o">=</span><span class="n">system</span><span class="p">,</span>
        <span class="n">state_init</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.14</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span>
        <span class="n">time_final</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">max_step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">policy_opt_method</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span>
    <span class="c1"># by default Adam minimizes objective, use {&quot;lr&quot;: 0.1, &quot;maximize&quot;: True} for rewards case</span>
    <span class="n">policy_opt_method_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
    <span class="n">running_objective</span><span class="o">=</span><span class="n">running_cost</span><span class="p">,</span>
    <span class="n">sampling_time</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">N_episodes</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">N_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">is_with_baseline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">is_do_not_let_the_past_distract_you</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Run the training process</span>
<span class="n">controller</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="proximal-policy-optimization-ppo-algorithm">
<h1>Proximal Policy Optimization (PPO) Algorithm<a class="headerlink" href="#proximal-policy-optimization-ppo-algorithm" title="Permalink to this headline"></a></h1>
<section id="id1">
<h2>Overview<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>The PPOController implements the Proximal Policy Optimization (PPO) algorithm, a policy gradient method for reinforcement learning that balances exploration and exploitation by clipping the policy update. PPO aims to take the biggest possible improvement step on a policy without causing performance collapse, thus ensuring a monotonic improvement.</p>
</section>
<section id="gradient-optimization-formulas">
<h2>Gradient Optimization Formulas<a class="headerlink" href="#gradient-optimization-formulas" title="Permalink to this headline"></a></h2>
<p>The optimization step in PPO is performed by solving a clipped surrogate objective function. The parameters for the policy are updated by minimizing the expected difference between the old and new policy while keeping the updates within a trust region.</p>
<section id="for-policy-updates">
<h3>For Policy Updates<a class="headerlink" href="#for-policy-updates" title="Permalink to this headline"></a></h3>
<p>The policy parameters are updated using the following formula:</p>
<div class="math notranslate nohighlight">
\[\theta_{i+1} \leftarrow \arg\min_{\theta}\mathbb{E}_{f, \rho^{\theta_i}}{\left[\sum_{k=0}^{N-1} \gamma^k \max\left(A^{\rho^{\theta_i}}(Y_k, U_k)\frac{\rho^{\theta}(U_k | Y_k)}{\rho^{\theta_i}(U_k | Y_k)}, A^{\rho^{\theta_i}}(Y_k, U_k) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\rho^{\theta}(U_k | Y_k)}{\rho^{\theta_i}(U_k | Y_k)}\right) \right]\right)}.\]</div>
<p>where <span class="math notranslate nohighlight">\(A^{\rho^{\theta_i}}(Y_k, U_k)\)</span> is the advantage function at timestep <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
</section>
<section id="id2">
<h2>Algorithm Steps<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Initialize policy parameters <span class="math notranslate nohighlight">\(\theta^1\)</span> and baseline function <span class="math notranslate nohighlight">\(J^w\)</span>.</p></li>
<li><p>For each iteration <span class="math notranslate nohighlight">\(i \in \{1, \ldots, N\_iterations\}\)</span>:</p>
<ol class="loweralpha simple">
<li><p>For each episode <span class="math notranslate nohighlight">\(j \in \{1, \ldots, M\}\)</span>:</p>
<ol class="lowerroman simple">
<li><p>For each time step <span class="math notranslate nohighlight">\(k \in \{0, \ldots, N-1\}\)</span>:</p>
<ul class="simple">
<li><p>Obtain observation <span class="math notranslate nohighlight">\(y_k^j\)</span> from the environment.</p></li>
<li><p>Sample action <span class="math notranslate nohighlight">\(u_k^j\)</span> from the policy <span class="math notranslate nohighlight">\(\rho^{\theta}(u_k^j | y_k^j)\)</span>.</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Update the baseline function by fitting <span class="math notranslate nohighlight">\(\hat{J}^{w}\)</span>.</p></li>
<li><p>Update the policy by performing a policy gradient step using the clipped surrogate objective function.</p></li>
</ol>
</li>
</ol>
</section>
<section id="id3">
<h2>Notation<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\_iterations\)</span>: Total number of training iterations.</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span>: Number of episodes per iteration.</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span>: Number of steps per episode.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>: Discount factor for future rewards.</p></li>
<li><p><span class="math notranslate nohighlight">\(A^{\rho^{\theta_i}}(Y_k, U_k)\)</span>: The advantage function, representing the relative value of action <span class="math notranslate nohighlight">\(U_k\)</span> in state <span class="math notranslate nohighlight">\(Y_k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho^{\theta}(u_k^j | y_k^j)\)</span>: The policy distribution from which actions are sampled.</p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\)</span>: The clipping hyperparameter that defines the trust region.</p></li>
</ul>
</section>
</section>
<section id="proximal-policy-optimization-algorithm">
<h1>Proximal Policy Optimization Algorithm<a class="headerlink" href="#proximal-policy-optimization-algorithm" title="Permalink to this headline"></a></h1>
<p>General formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boxed{
  \begin{array}{l}
  \hphantom{~}
  \\
  \theta_{i+1} \leftarrow \arg\min_{\theta}\mathbb{E}_{f, \rho^{\theta_i}}{\sum_{k=0}^{\infty} \gamma ^ k \max\left(A^{\rho^{\theta_i}}(Y_k, U_k)   \frac{\rho^{\theta}(U_k \mid Y_k)}{\rho^{\theta_i}(U_k \mid Y_k)}, A^{\rho^{\theta_i}}(Y_k, U_k) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\rho^{\theta}(U_k \mid Y_k)}{\rho^{\theta_i}(U_k \mid Y_k)}\right) \right)}
  \\
  \hphantom{~}
  \end{array}
}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A^{\rho^{\theta_i}}(Y_k, U_k) = r(Y_k, U_k) + \gamma J^{\rho^{\theta_i}}(Y_{k+1}) - J^{\rho^{\theta_i}}(Y_{k})\)</span>.
On practice the number of steps <span class="math notranslate nohighlight">\(k\)</span> is finite and we denote it by <span class="math notranslate nohighlight">\(N\)</span>. The detailed description of algorithm is as follows
(note that it works only for <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>).</p>
<ol class="arabic">
<li><p>For each iteration <span class="math notranslate nohighlight">\(i\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots, N\_iterations\}\)</span>:</p>
<ol class="loweralpha simple">
<li><p>For each episode <span class="math notranslate nohighlight">\(j\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots, M\}\)</span>:</p>
<ol class="lowerroman simple">
<li><p>For each time step <span class="math notranslate nohighlight">\(k\)</span> in <span class="math notranslate nohighlight">\(\{0, \ldots, N-1\}\)</span>:</p>
<ul class="simple">
<li><p>Obtain observation <span class="math notranslate nohighlight">\(y_k^j\)</span> from the environment.</p></li>
<li><p>Sample action <span class="math notranslate nohighlight">\(u_k^j\)</span> from a truncated normal distribution that is truncated to action bounds (provided as default parameters), using the policy <span class="math notranslate nohighlight">\(\rho^{\theta}(u_k^j | y_k^j)\)</span>.</p></li>
</ul>
</li>
</ol>
</li>
</ol>
<p>b. Now we need to fit cost-to-go <span class="math notranslate nohighlight">\(\hat{J}^{w}\)</span>. We can do it by minimizing temporal
difference loss with learning rate <span class="math notranslate nohighlight">\(\eta\)</span> (note that <span class="math notranslate nohighlight">\(N_{\text{TD}}\)</span>, <span class="math notranslate nohighlight">\(N_{\text{epochs}}^{\text{Critic}}\)</span>
are also hyperparameters of algorithm).
The optimization procedure converges only for <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>:</p>
<blockquote>
<div><ol class="lowerroman">
<li><p>For each epoch in <span class="math notranslate nohighlight">\(e\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots,N_{\text{epochs}}^{\text{Critic}}\)</span>}`</p>
<blockquote>
<div><ul>
<li><p>For each episode <span class="math notranslate nohighlight">\(j\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots, M\}\)</span>:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w^{\text{new}} \leftarrow w^{\text{old}} - \eta \nabla_{w}\text{TDLoss}\)</span></p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ol>
</div></blockquote>
<p>c. Perform a policy gradient optimization procedure: For current policy weights <span class="math notranslate nohighlight">\(\theta_{i}\)</span>
calculate <span class="math notranslate nohighlight">\(\rho^{\theta_{i}}(u^j_k | y^j_k)\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(k\)</span></p>
<blockquote>
<div><ol class="lowerroman">
<li><p>For each epoch in <span class="math notranslate nohighlight">\(e\)</span> in <span class="math notranslate nohighlight">\(\{1, \ldots,N_{\text{epochs}}^{\text{Policy}}\)</span>}`:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta^{\text{new}}\leftarrow \theta^{\text{old}}-\alpha\nabla_{\theta}\text{PolicyObjective}\)</span></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</li>
</ol>
<p>In the listing above we denote <span class="math notranslate nohighlight">\(\text{TDLoss}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\sum\limits_{k = 0}^{N-1-N_{\text{TD}}} \left(\hat{J}^{w}\left(y^j_k\right) - r\left(y^j_k, u_k^j\right)  -... - \gamma^{N_{\text{TD}}-1} r\left(y^j_{k + N_{\text{TD}}-1}, u^j_{k + N_{\text{TD}}-1}\right) - \gamma^{N_{\text{TD}}} \hat{J}^{w}\left(y^j_{k + N_{\text{TD}}}\right)\right) ^ 2}{N-1-N_{\text{TD}}},\]</div>
<p>and <span class="math notranslate nohighlight">\(\text{PolicyObjective}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{M}\sum\limits_{j=1}^{M}\sum\limits_{k=0}^{N-2}\gamma^k \max\left(\hat{A}^{w}(y^j_k, u^j_k)   \frac{\rho^{\theta}(u^j_k \mid y^j_k)}{\rho^{\theta_{i}}(u^j_k \mid y^j_k)}, \hat{A}^{w}(y^j_k, u^j_k) \operatorname{clip}_{1 - \varepsilon}^{1 + \varepsilon}\left(\frac{\rho^{\theta}(u^j_k \mid y^j_k)}{\rho^{\theta_{i}}(u^j_k \mid y^j_k)}\right) \right).\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">regelum</span> <span class="k">as</span> <span class="nn">rg</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">regelum.system</span> <span class="kn">import</span> <span class="n">InvertedPendulumPD</span>
<span class="kn">from</span> <span class="nn">regelum.objective</span> <span class="kn">import</span> <span class="n">RunningObjective</span>
<span class="kn">from</span> <span class="nn">regelum.simulator</span> <span class="kn">import</span> <span class="n">CasADi</span>

<span class="n">system</span> <span class="o">=</span> <span class="n">InvertedPendulumPD</span><span class="p">()</span>
<span class="n">controller</span> <span class="o">=</span> <span class="n">rg</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">PPOController</span><span class="p">(</span>
    <span class="n">policy_model</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">PerceptronWithTruncatedNormalNoise</span><span class="p">(</span>
        <span class="n">dim_input</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">dim_observation</span><span class="p">,</span>
        <span class="n">dim_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">dim_output</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">dim_inputs</span><span class="p">,</span>
        <span class="n">hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">output_activation</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">MultiplyByConstant</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">100</span><span class="p">),</span>
        <span class="n">output_bounds</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">action_bounds</span><span class="p">,</span>
        <span class="n">stds</span><span class="o">=</span><span class="p">[[</span><span class="mf">2.5</span><span class="p">]],</span>
        <span class="n">is_truncated_to_output_bounds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">critic_model</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModelPerceptron</span><span class="p">(</span>
        <span class="n">dim_input</span><span class="o">=</span><span class="n">system</span><span class="o">.</span><span class="n">dim_observation</span><span class="p">,</span>
        <span class="n">dim_hidden</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">dim_output</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">simulator</span><span class="o">=</span><span class="n">CasADi</span><span class="p">(</span>
        <span class="n">system</span><span class="o">=</span><span class="n">system</span><span class="p">,</span>
        <span class="n">state_init</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.14</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span>
        <span class="n">time_final</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">max_step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">critic_n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">policy_n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">critic_opt_method_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">},</span>
    <span class="n">policy_opt_method_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
    <span class="n">sampling_time</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">running_objective</span><span class="o">=</span><span class="n">RunningObjective</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">rg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ModelQuadLin</span><span class="p">(</span><span class="s2">&quot;diagonal&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">controller</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tutorials.html" class="btn btn-neutral float-left" title="Configs and basic regelum applications" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="systems_tutorial.html" class="btn btn-neutral float-right" title="Tutorial: Implementing Custom Systems in Regelum" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, AIDA Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>