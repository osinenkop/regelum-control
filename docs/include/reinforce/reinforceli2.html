<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Description</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="reinforce.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="description" link="x3-2000" md-heading="Description" name="md-heading" type="h3"/></head><body>
<!-- l. 31 -->
<!-- l. 32 --><p class="noindent">The underlying theory of the REINFORCE algorithm is captured by the relationship
between the gradient of the expected total cost and the policy parameters \(\theta \). This
relationship is formally stated by the policy gradient theorem, which is mathematically
represented as \begin {equation}  \label {eqn_reinforce_log_lh_trick} \begin {aligned} &amp;\nabla _{\theta } \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{T-1} \gamma ^ t \Cost (\State _{t}, \Action _{t})} = \\ &amp;\quad \E [\transit , \policy ^{\theta }]{ \sum _{t = 0}^{T-1}\left (\sum _{t' = 0}^{T-1} \gamma ^ {t'} \Cost (\State _{t'}, \Action _{t'})\right ) \nabla _{\theta }\log \policy ^{\theta }(\Action _t \mid \State _t)} \end {aligned}  \end {equation}<a id="x3-2001r2"></a>
</p>
<!-- l. 90 -->
<!-- l. 90 --><p class="indent"> <a id="tailreinforceli2.html"></a> </p>
</body>
</html>