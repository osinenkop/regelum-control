<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Problem statement</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="reinforce.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="problem-statement" link="x2-1000" md-heading="Problem statement" name="md-heading" type="h3"/></head><body>
<!-- l. 6 -->
<!-- l. 7 --><p class="noindent">REINFORCE is a policy gradient algorithm that solves the following problem:
</p><!-- l. 12 --><p class="indent">   \begin {equation}  \label {eqn_reinforce_problem} \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{T-1} \gamma ^ t \Cost (\State _{t}, \Action _{t})} \ra \min _{\theta },  \end {equation}<a id="x2-1001r1"></a> where:
      </p><ol>
<li><p>\(\gamma \in [0, 1]\) is the  <span class="cmti-12">discount factor </span> of the algorithm and is usually set to 1 in REINFORCE.
      </p></li><li><p>\(\transit : \states \times \actions \times \states \ \rightarrow \ \R _{\geq 0}\) is the  <span class="cmti-12">transition probability density function </span> of the environment. The function
      \(\transit (\bullet \mid \state , \action )\) is the probability density of the next state conditioned on the current state \(\state \)
      and the current action \(\action \):
                                                                                   
                                                                                   
                                            \[ \State _{t+1} \sim \transit (\bullet \mid \State _t, \Action _t), \quad \text {for all timesteps } t \in \{0, 1, \dots , T-1\} \]
      It is also assumed that the initial state \(\State _0\) for \(t = 0\) is sampled from some distribution
      with an unconditional density \(\transit _0 : \states \ \rightarrow \ \R _{\geq 0}\):
                                            \[ \State _0 \sim \transit _0(\bullet ) \]
      </p></li><li><p>\(\policy ^{\theta } : \actions \times \states \rightarrow \R _{\geq 0}\) is the  <span class="cmti-12">stochastic policy </span> of the agent that is parameterized by weights \(\theta \). Precicely,
      \(\policy ^{\theta }(\bullet \mid \state )\) is the probability density of the action conditioned on the current state \(\state \):
                                            \[ \Action _t \sim \policy ^{\theta }(\bullet \mid \State _t) \]
      </p></li><li><p>\(\Cost (s, a)\) denotes the cost associated with the current state \(\state \) and action \(\action \)</p></li>
</ol>
<!-- l. 31 -->
<!-- l. 31 --><p class="indent"> <a id="tailreinforceli1.html"></a> </p>
</body>
</html>