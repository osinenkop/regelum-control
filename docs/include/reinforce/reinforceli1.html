<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Full listing of the algorithm</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="reinforce.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="full-listing-of-the-algorithm" link="x2-1000" md-heading="Full listing of the algorithm" name="md-heading" type="h3"/></head><body>
<!-- l. 6 -->
<p><a id="x2-1001r1"></a>
</p><!-- l. 8 --><p class="indent"> </p><figure class="float" id="x2-1002r1"><span id="reinforce"></span><span></span>
<span class="cmbx-12">Algorithm 1:</span> REINFORCE
   
   <a id="x2-1003"></a>
</figure><div class="algorithmic">
<a id="x2-1004r1"></a>
<span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> 
      </span><span class="cmbx-12">Input:</span>
      \(\theta _1\)
      (initial
      policy
      weights)
      <a id="x2-1005r2"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">for</span> learning iteration \(i := 1 \dots \mathcal I\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x2-1006r3"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">for</span> episode \(j := 1 \dots M\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x2-1007r4"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;"> </span>
          obtain
          initial
          state
          \(\State _0^{j}\)
          <a id="x2-1008r5"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">for</span> step \(t := 0 \dots T - 1\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x2-1009r6"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            sample
            action
            \(\Action _t^j \sim \policy ^{\theta }(\bullet \mid \State _t^{j})\)
            <a id="x2-1010r7"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            obtain
            state
            from
            transition
            function
            \(\State _{t+1}^j \sim \transit (\bullet \mid \State _t^j, \Action _t^j)\)
          </span><a id="x2-1011r8"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span>
</span><a id="x2-1012r9"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x2-1013r10"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Perform a policy gradient step: \begin {equation}  \label {eqn_reinforce_update_rule} \theta _{i+1} \la \theta _i - \alpha \frac {1}{M}\sum _{j = 1}^M \sum _{t = 0}^{T-1}\sum _{t'=t}^{T-1}\left ( \gamma ^{t'} \Cost (\State _{t'}^j, \Action _t^j) - B_{t}^i\right ) \nabla _{\theta }\log \pi ^{\theta }(A_t^j \mid \State _t^j)\rvert _{\theta = \theta _i},  \end {equation}<a id="x2-1014r1"></a> <a id="x2-1015r11"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   where we describe the all entities in the formula right after the algorithm
      </span><a id="x2-1016r12"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x2-1017r13"></a>
<a id="x2-1018r14"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> </span>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">return </span> Optimal policy \(\policy ^{\theta _{\mathcal I}}\)
   </div>
<!-- l. 31 --><p class="noindent">In equation \eqref{eqn_reinforce_update_rule} \(\gamma \) is the discount factor, \(\alpha \) is the learning rate, and \(\Cost (S_{t'}, A_t)\) denotes the cost
associated with state \(\State _{t'}\) and action \(\Action _t\). The term \(B_t^i\) is known as the baseline, which is a
random variable conditionally independent of \(A_t^j\). The baseline can be any arbitrary
function of the state \(\State _t^j\) (\(B_t^i = f(\State _t^j)\)), such as the value function (\(B_t^i = \hat \Value (\State _t^j)\)) or any random variable. For
instance we can put it as tail total costs from previous iteration (and we do it in
regelum):
                                         \[ B_t^{i + 1} = \sum _{t'=t}^{T-1}\gamma ^{t'} \Cost (\State _{t'}^j, \Action _t^j), \]
 and for the first iteration \(i = 1\) we can set \(B_0^1 = 0\).
                                                                                   
                                                                                   
</p>
<!-- l. 37 -->
<!-- l. 37 --><p class="indent"> <a id="tailreinforceli1.html"></a> </p>
</body>
</html>