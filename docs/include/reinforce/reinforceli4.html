<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Baseline and ”do not let the past distract you” principle</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="reinforce.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="baseline-and-do-not-let-the-past-distract-you-principle" link="x5-4000" md-heading="Baseline and ”do not let the past distract you” principle" name="md-heading" type="h4"/></head><body>
<!-- l. 65 -->
<!-- l. 66 --><p class="noindent">Nevertheless, the described update approach on pratice suffers from high variance (even for
large \(M\)), leading to inefficient gradient descent steps. To mitigate this issue, the equation \eqref{eqn_reinforce_log_lh_trick}
can be improved as follows: \begin {equation}  \label {eqn_reinforce_log_lh_trick_with_baseline_and_tail_costs} \begin {aligned} &amp;\nabla _{\theta } \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{T-1} \gamma ^ t \Cost (\State _{t}, \Action _{t})} = \\ &amp;\quad \E [\transit , \policy ^{\theta }]{ \sum _{t = 0}^{T-1} \left ( \sum _{t' = t}^{T-1} \gamma ^ {t'} \Cost (\State _{t'}, \Action _{t'}) - B_t \right ) \nabla _{\theta }\log \policy ^{\theta }(\Action _t \mid \State _t)}, \end {aligned}  \end {equation}<a id="x5-4001r3"></a> where \(B_t\) is the <span class="cmti-12">baseline</span>, a random variable independent of action
\(\Action _t\) <a href="https://en.wikipedia.org/wiki/Conditional_independence#Conditional_independence_of_random_vectors">conditioned on</a> \(\State _t\).
</p><!-- l. 77 --><p class="indent">   Note that in formula \eqref{eqn_reinforce_log_lh_trick_with_baseline_and_tail_costs}, as compared to \eqref{eqn_reinforce_log_lh_trick}, we incorporate two significant
modifications to improve the formulation. Firslty, we introduce a variance reduction
technique by subtracting a baseline \(B_t\). Secondly, the full total cost \(\sum _{t' = 0}^{T-1} \gamma ^ {t'} \Cost (\State _{t'}, \Action _{t'})\) is replaced by the tail
total cost \(\sum _{t' = t}^{T-1} \gamma ^ {t'} \Cost (\State _{t'}, \Action _{t'})\) (the approach is called <span class="cmti-12">do not let the past distract you</span>).
</p><!-- l. 81 --><p class="indent">   The baseline function can be <span class="cmti-12">any </span>Borel measurable function of the state \(\State _t\) (such as the
estimated value function \(B_t = \hat {\Value }(\State _t)\)), or it could be any other appropriate random variable that is
independent of the action \(\Action _t\). For example, one could consider the averaged tail total
costs from the prior iteration of learning. Specifically, let us denote by iteration \(i\)
the process where one obtains state-action pairs \((\State _t^j, \Action _t^j)\) for \(t = 0, 1, \ldots , T - 1\) and \(j = 1, 2, \ldots , M\) through Monte Carlo
simulations. The baselines \(B_t^{i+1}\) for the subsequent iteration \(i+1\) can then be computed
as
                                         \[ B_{t}^{i + 1} = \frac {1}{M}\sum _{j = 1}^M \sum _{t' = t}^{T-1} \gamma ^{t'} \Cost (\State ^j_{t'}, \Action ^j_{t'}), \]
 Finally, we come up with the final version of REINFORCE algorithm in the subsequent
section.
                                                                                   
                                                                                   
</p>
<!-- l. 90 -->
<!-- l. 90 --><p class="indent"> <a id="tailreinforceli4.html"></a> </p>
</body>
</html>