<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Description</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ddpg.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="description" link="x4-3000" md-heading="Description" name="md-heading" type="h3"/></head><body>
<!-- l. 45 -->
<!-- l. 46 --><p class="noindent">The underlying theory of the DDPG algorithm is captured by the relationship between the
gradient of the expected total cost and the policy parameters \(\theta \). This relationship is formally
stated by the policy gradient theorem, which is mathematically represented as
\begin {equation}  \label {eqn_ddpg_grad} \nabla _{\theta } \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} = \E [\transit , \policy ^{\theta }]{\sum _{t=0}^{\infty }\gamma ^t\nabla _{\theta }\policy ^{\theta }(\State _t)\nabla _{\action } Q^{\policy ^{\theta }}(\State _t, \action )\big \rvert _{\action = \Action _t}}  \end {equation}<a id="x4-3001r2"></a>
</p><!-- l. 54 --><p class="indent">   In practice, \eqref{eqn_ddpg_grad} can be approximated using Monte Carlo methods. Specifically, by
sampling \(M\) trajectories (referred to as <span class="cmti-12">episodes</span>), which are generated using the policy \(\pi ^{\theta }\), and
then averaging across these episodes, we obtain the following approximation: \begin {equation}  \label {eqn_ddpg_objective_est0} \begin {aligned} &amp;\E [\transit , \policy ^{\theta }]{\sum _{t=0}^{\infty }\gamma ^t\nabla _{\theta }\policy ^{\theta }(\State _t)\nabla _{\action } Q^{\policy ^{\theta }}(\State _t, \action )\big \rvert _{\action = \Action _t}} \approx \\ &amp;\quad \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \nabla _{\theta }\policy ^{\theta }(\State _t^j)\nabla _{\action } Q^{\policy ^{\theta }}(\State _t, \action )\big \rvert _{\action = \Action _t^j}, \end {aligned}  \end {equation}<a id="x4-3002r3"></a> where \(T\)
denotes the length of an episode, and \(\State _t^j\) and \(\Action _t^j\) represent the state and action at time \(t\) of the
\(j\)-th episode, respectively. However, the Q-function \(Q^{\policy ^{\theta }}(s, a)\) is generally unknown and must
be estimated. This estimation can be performed by minimizing the temporal
difference (TD) loss for the Q-function \(Q^{\policy ^{\theta }}\), as described by the following equation: \begin {equation}  \label {eqn_td_loss} \sum _{j=1}^M\sum _{t=0}^{T-1 - N_{\text {TD}}}\left (\hat Q^w(\State _t^j, \Action _t^j) - \sum _{t'=t}^{t + N_{\text {TD}} - 1} \gamma ^{t'-t}\Cost (\State _{t'}^j, \Action _{t'}^j) - \gamma ^{N_{\text {TD}}}\hat Q^w(\State _{t + N_{\text {TD}}}^j, \Action _{t + N_{\text {TD}}}^j)\right )^2,  \end {equation}<a id="x4-3003r4"></a>
where \(N_{\text {TD}}\) is the TD window, a hyperparameter. After optimization, we achieve an
approximation of the Q-function, \(Q^{\pi _{\theta }} \approx \hat {Q}^w\). Substituting this estimated Q-function \(\hat {Q}^w\) into \eqref{eqn_ddpg_objective_est0}
yields \begin {equation}  \label {eqn_ddpg_objective_est} \begin {aligned} &amp;\E [\transit , \policy ^{\theta }]{\sum _{t=0}^{\infty }\gamma ^t\nabla _{\theta }\policy ^{\theta }(\State _t)\nabla _{\action } Q^{\policy ^{\theta }}(\State _t, \action )\big \rvert _{\action = \Action _t}} \approx \\ &amp;\quad \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \nabla _{\theta }\policy ^{\theta }(\State _t^j)\nabla _{\action } \hat Q^{w}(\State _t, \action )\big \rvert _{\action = \Action _t^j}. \end {aligned}  \end {equation}<a id="x4-3004r5"></a> We come up with the final version of DDPG algorithm in the subsequent
section.
                                                                                   
                                                                                   
</p>
<!-- l. 78 -->
<!-- l. 78 --><p class="indent"> <a id="tailddpgli3.html"></a> </p>
</body>
</html>