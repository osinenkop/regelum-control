<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Problem statement</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ddpg.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="problem-statement" link="x2-1000" md-heading="Problem statement" name="md-heading" type="h3"/></head><body>
<!-- l. 5 -->
<!-- l. 6 --><p class="noindent">Deep Deterministic Policy Gradient (DDPG) is a policy gradient algorithm that solves the
following problem:
</p><!-- l. 11 --><p class="indent">   \begin {equation}  \label {eqn_ddpg_problem} \E [\transit , \policy ^{\theta }]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} \ra \min _{\theta },  \end {equation}<a id="x2-1001r1"></a> where:
      </p><ol>
<li><p>\(\gamma \in (0, 1)\) is the  <span class="cmti-12">discount factor </span> of the algorithm.
      </p></li><li><p>\(\transit : \states \times \actions \times \states \ \rightarrow \ \R _{\geq 0}\) is the  <span class="cmti-12">transition probability density function </span> of the environment. Precicely, \(\transit (\bullet \mid \state , \action )\) is
      the probability density of the next state conditioned on the current state \(\state \) and
      the current action \(\action \):
                                                                                   
                                                                                   
                                            \[ \State _{t+1} \sim \transit (\bullet \mid \State _t, \Action _t), \quad \text {for all timesteps } t \in \{0, 1, \dots , T-1\} \]
      It is also assumed that the initial state \(\State _0\) for \(t = 0\) is sampled from some distribution
      with an unconditional density \(\transit _0 : \states \ \rightarrow \ \R _{\geq 0}\):
                                            \[ \State _0 \sim \transit _0(\bullet ) \]
      </p></li><li><p>\(\policy ^{\theta } : \states \rightarrow \R _{\geq 0}\) is the  <span class="cmti-12">deterministic policy </span> of the agent that is paramenterized by weights \(\theta \).
      Precicely,
                                            \[ \Action _t = \policy ^{\theta }(\State _t) \]
      for all \(t \in \{0, 1, 2, \dots \}\)
      </p></li><li><p>\(\Cost (\state , \action )\) denotes the cost associated with the current state \(\state \) and action \(\action \)</p></li>
</ol>
<!-- l. 31 -->
<!-- l. 31 --><p class="indent"> <a id="tailddpgli1.html"></a> </p>
</body>
</html>