<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>VPG algorithm</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="vpg.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="vpg-algorithm" link="x5-4000" md-heading="VPG algorithm" name="md-heading" type="h3"/></head><body>
<!-- l. 81 -->
<p><a id="x5-4001r1"></a>
</p><!-- l. 83 --><p class="indent"> </p><figure class="float" id="x5-4002r1"><span id="vanilla-policy-gradient-vpg"></span><span></span>
<span class="cmbx-12">Algorithm 1:</span> Vanilla Policy Gradient (VPG)
   
   <a id="x5-4003"></a>
</figure><div class="algorithmic">
<a id="x5-4004r1"></a>
<span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> 
      </span><span class="cmbx-12">Input:</span>
      \(\theta _1\)
      (initial
      policy
      weights)
      <a id="x5-4005r2"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">for</span> learning iteration \(i := 1 \dots \mathcal I\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x5-4006r3"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">for</span> episode \(j := 1 \dots M\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x5-4007r4"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;"> </span>
          obtain
          initial
          state
          \(\State _0^{j}\)
          <a id="x5-4008r5"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">for</span> step \(t := 0 \dots T - 1\) <span class="cmbx-12">do</span><span class="for-body">
<a id="x5-4009r6"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            sample
            action
            \(\Action _t^j \sim \policy ^{\theta }(\bullet \mid \State _t^{j})\)
            <a id="x5-4010r7"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:41.12457pt;"> </span>
            obtain
            state
            from
            transition
            function
            \(\State _{t+1}^j \sim \transit (\bullet \mid \State _t^j, \Action _t^j)\)
          </span><a id="x5-4011r8"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:29.3747pt;">       </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span>
</span><a id="x5-4012r9"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;">    </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x5-4013r10"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Optimize critic \(\hat \Value ^{w}\) using gradient descent to minimize the temporal difference loss \eqref{eqn_td_loss}
        <a id="x5-4014r11"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Estimate the advantages \(\hat \Advan ^{w}(\State _t, \Action _t)\) as described in \eqref{eqn_advan_est} <a id="x5-4015r12"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:17.62482pt;"> </span>   Perform a policy gradient step:
                                               \[ \theta _{i+1} \la \theta _i - \frac {1}{M} \sum _{j = 1}^M \sum _{t=0}^{T-1} \gamma ^ t \hat \Advan ^{w}(\State _t^j, \Action _t^j) \nabla _{\theta } \log \policy ^{\theta }(\Action _t^j \mid \State _t^j)\rvert _{\theta = \theta _i}. \]
      </span><a id="x5-4016r13"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">end</span> <span class="cmbx-12">for</span><a id="x5-4017r14"></a>
<a id="x5-4018r15"></a>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;"> </span>
<br/><span class="ALCitem"></span><span class="ALIndent" style="width:5.87494pt;">  </span><span class="cmbx-12">return </span> Optimal policy \(\policy ^{\theta _{\mathcal I}}\)
   </div>
<!-- l. 107 -->
<!-- l. 107 --><p class="indent"> <a id="tailvpgli4.html"></a> </p>
</body>
</html>