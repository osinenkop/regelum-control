<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Motivation</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ppo.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="motivation" link="x4-3000" md-heading="Motivation" name="md-heading" type="h3"/></head><body>
<!-- l. 50 -->
<!-- l. 52 --><p class="noindent">Proximal Policy Optimization (PPO) evolved from Trust Region Policy Optimization
(TRPO), and to understand the motivation behind the algorithm, we begin by considering
a key theoretical result from  <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a>.
</p><!-- l. 54 --><p class="indent">   Let us consider the following update rule:
                                         \[ \theta _{i + 1} \la \arg \min _{\theta } \left ( \E [\transit , \policy ^{\theta _i}]{\sum _{t=0}^{\infty } \gamma ^ t \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t \mid \State _t)}} + C_i d_{\text {KL}}^{\max }\left (\policy ^{\theta _i}\;\middle \|\;\policy ^{\theta }\right ) \right ), \]
 where
      </p><ol>
<li><p>\(i\) is the number of learning iteration
      </p></li><li><p>\(C_i := \frac {4 \gamma \max _{s \in \states , a \in \actions }|\Advan ^{\policy ^{\theta _i}}(s, a)|}{(1 - \gamma )^2}\)
      </p></li><li><p>\(d_{\text {KL}}^{\max }\left (\policy ^{\theta _i}\;\middle \|\;\policy ^{\theta }\right ) = \max _{\state \in \states }\kldiv {\policy ^{\theta _i}(\bullet \mid s)}{\policy ^{\theta }(\bullet \mid s)}\)</p></li>
</ol>
<!-- l. 64 --><p class="noindent">According to this rule, it is guaranteed that the expected cumulative cost does not increase
with each iteration:
                                                                                   
                                                                                   
                                         \[ \E [\transit , \policy ^{\theta _1}]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} \geq \E [\transit , \policy ^{\theta _2}]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} \geq \E [\transit , \policy ^{\theta _3}]{\sum _{t = 0}^{\infty } \gamma ^ t \Cost (\State _{t}, \Action _{t})} \geq ... \]
 Despite these guarantees, the straighforward application of this approach is faced with
challenges such as computational complexity and slow convergence rates. Nevertheless, we
can derive significant interpretation of the result: when the optimization steps are
sufficiently small, such that \(\theta _i \rightarrow \theta _{i+1}\) yields a minimal increase in the maximal Kullback-Leibler
divergence \(d_{\text {KL}}^{\max }\left (\policy ^{\theta _i} \,\middle \|\, \policy ^{\theta _{i + 1}}\right )\), and while the objective \[ \E [\transit , \policy ^{\theta _i}]{\sum _{t=0}^{\infty } \gamma ^ t \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t \mid \State _t)}} \] is minimized, each subsequent iteration is ensured
to be at least as good as the previous one. </p><!-- l. 74 -->
<!-- l. 74 --><p class="indent"> <a id="tailppoli3.html"></a> </p>
</body>
</html>