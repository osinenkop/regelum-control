<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Update rule</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ppo.tex" name="src"/>
<script>window.MathJax = { tex: { tags: "ams", }, }; </script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<meta id="update-rule" link="x5-4000" md-heading="Update rule" name="md-heading" type="h3"/></head><body>
<!-- l. 74 -->
<!-- l. 75 --><p class="noindent">To address practical difficulties and incorporate the proposed interpretation, Proximal
Policy Optimization (PPO) introduces the following update rule: \begin {equation}  \theta _{i+1} \la \arg \min _{\theta }\E [\transit , \policy ^{\theta _i}]{\sum _{t=0}^{\infty } \gamma ^ t \max \left (\Advan _t^{\policy ^{\theta _i}} r_t(\theta , \theta _i), \Advan _t^{\policy ^{\theta _i}}\operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (r_t(\theta , \theta _i) \right )\right )}  \end {equation}<a id="x5-4001r2"></a> where
      </p><ol>
<li><p>\(\varepsilon \) is a hyperparameter, typically set to \(\varepsilon = 0.2\)
      </p></li><li><p>\(r_t(\theta , \theta _i) = \frac {\policy ^{\theta }(\Action _t \mid \State _t)}{\policy ^{\theta _i}(\Action _t \mid \State _t)}\)
      </p></li><li><p>\(\Advan _t^{\policy ^{\theta _i}} = \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t)\)</p></li>
</ol>
<!-- l. 85 --><p class="noindent">The rationale behind this objective function is as follows:
</p><!-- l. 87 --><p class="indent">   The first term within the \(\max \) expression, \begin {equation}  \Advan _t^{\policy ^{\theta _i}} r_t(\theta , \theta _i) = \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \frac {\policy ^{\theta }(\Action _t | \State _t)}{\policy ^{\theta _i}(\Action _t | \State _t)},  \end {equation}<a id="x5-4005r3"></a> is adopted from Trust Region Policy
Optimization (TRPO). The second term, \begin {equation}  \Advan _t^{\policy ^{\theta _i}}\operatorname {clip}_{1 - \varepsilon }^{1 + \varepsilon }\left (r_t(\theta , \theta _i) \right ) = \Advan ^{\policy ^{\theta _i}}(\State _t, \Action _t) \operatorname {clip}_{1-\varepsilon }^{1 + \varepsilon }\left (\frac {\policy ^{\theta }(\Action _t | \State _t)}{\policy ^{\theta _i}(\Action _t | \State _t)}\right ),  \end {equation}<a id="x5-4006r4"></a> modifies the objective by clipping the probability
ratio to keep it within the interval \([1 - \varepsilon , 1 + \varepsilon ]\). This modification removes the incentive for moving the
ratio \(\frac {\policy ^{\theta }(\Action _t | \State _t)}{\policy ^{\theta _i}(\Action _t | \State _t)}\) outside of this interval.
</p><!-- l. 97 --><p class="indent">   Finally, the maximum between the clipped and unclipped values is computed so that
the final objective serves as a conservative or pessimistic bound on the unclipped objective.
This approach ensures that changes in the probability ratio are considered only when they
do not lead to an improvement in the objective, thereby providing a safeguard against
overly aggressive updates.
                                                                                   
                                                                                   
</p>
<!-- l. 99 -->
<!-- l. 99 --><p class="indent"> <a id="tailppoli4.html"></a> </p>
</body>
</html>